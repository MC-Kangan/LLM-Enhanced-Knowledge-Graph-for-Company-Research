{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-28 18:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenkangan/Documents/UCL DSML/Thesis/project/ucl_project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl_scraping import *\n",
    "from utility import *\n",
    "from llm_extraction import *\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import tiktoken as tiktoken\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "print(current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gpt-4o: \"o200k_base\",\n",
    "- gpt-4: \"cl100k_base\",\n",
    "- gpt-3.5-turbo: \"cl100k_base\",\n",
    "- gpt-3.5: \"cl100k_base\",  # Common shorthand\n",
    "- gpt-35-turbo : \"cl100k_base\",  # Azure deployment name\n",
    "\n",
    "gpt-4o US$5.00 / 1M input tokens； US$15.00 / 1M output tokens\n",
    "\n",
    "gpt-4o context length: 128K tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>companies</th>\n",
       "      <th>company_former_name</th>\n",
       "      <th>company_legal_name</th>\n",
       "      <th>competitors</th>\n",
       "      <th>description</th>\n",
       "      <th>primary_industry_sector</th>\n",
       "      <th>primary_industry_group</th>\n",
       "      <th>primary_industry_code</th>\n",
       "      <th>all_industries</th>\n",
       "      <th>...</th>\n",
       "      <th>first_financing_valuation</th>\n",
       "      <th>first_financing_valuation_status</th>\n",
       "      <th>last_financing_valuation</th>\n",
       "      <th>last_financing_valuation_status</th>\n",
       "      <th>last_known_valuation</th>\n",
       "      <th>last_known_valuation_date</th>\n",
       "      <th>last_known_valuation_deal_type</th>\n",
       "      <th>processed_url</th>\n",
       "      <th>is_accessible</th>\n",
       "      <th>processed_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55185-04</td>\n",
       "      <td>Estimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estimize, Inc.</td>\n",
       "      <td>Neudata, SigFig, Motif (Financial Software), Y...</td>\n",
       "      <td>Developer of an open financial estimates platf...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Financial Software</td>\n",
       "      <td>Financial Software*, Media and Information Ser...</td>\n",
       "      <td>...</td>\n",
       "      <td>6.34</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.00</td>\n",
       "      <td>16/07/2015</td>\n",
       "      <td>Early Stage VC</td>\n",
       "      <td>www.estimize.com</td>\n",
       "      <td>True</td>\n",
       "      <td>estimize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56288-62</td>\n",
       "      <td>New Constructs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Constructs, LLC</td>\n",
       "      <td>Morningstar, CFRA, Finbox (Media and Informati...</td>\n",
       "      <td>Operator of an investment research firm intend...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Financial Software</td>\n",
       "      <td>Financial Software*, Media and Information Ser...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.17</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>13/05/2003</td>\n",
       "      <td>Early Stage VC</td>\n",
       "      <td>www.newconstructs.com</td>\n",
       "      <td>True</td>\n",
       "      <td>new_constructs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53739-01</td>\n",
       "      <td>Procore Technologies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Procore Technologies, Inc.</td>\n",
       "      <td>Projectmates, eBuilder, CMiC</td>\n",
       "      <td>Procore Technologies Inc is a cloud-based cons...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Business/Productivity Software</td>\n",
       "      <td>Business/Productivity Software*, Construction ...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Actual</td>\n",
       "      <td>8585.03</td>\n",
       "      <td>Estimated</td>\n",
       "      <td>8585.03</td>\n",
       "      <td>20/05/2021</td>\n",
       "      <td>IPO</td>\n",
       "      <td>www.procore.com</td>\n",
       "      <td>True</td>\n",
       "      <td>procore_technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153145-27</td>\n",
       "      <td>Proof</td>\n",
       "      <td>16 Pins, Notarize</td>\n",
       "      <td>Notarize, Inc.</td>\n",
       "      <td>Templafy, ZorroSign, eOriginal, PandaDoc, Cong...</td>\n",
       "      <td>Developer of an identity-assured transaction m...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Business/Productivity Software</td>\n",
       "      <td>Business/Productivity Software*, Media and Inf...</td>\n",
       "      <td>...</td>\n",
       "      <td>46.50</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>760.00</td>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>Later Stage VC</td>\n",
       "      <td>www.proof.com</td>\n",
       "      <td>True</td>\n",
       "      <td>proof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52304-77</td>\n",
       "      <td>SMS Assist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SMS Assist, L.L.C.</td>\n",
       "      <td>ServiceChannel, Divisions Maintenance Group, T...</td>\n",
       "      <td>Provider of business services intended to deli...</td>\n",
       "      <td>Business Products and Services (B2B)</td>\n",
       "      <td>Commercial Services</td>\n",
       "      <td>Other Commercial Services</td>\n",
       "      <td>Buildings and Property, Business/Productivity ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>950.00</td>\n",
       "      <td>Estimated</td>\n",
       "      <td>950.00</td>\n",
       "      <td>05/01/2023</td>\n",
       "      <td>Merger/Acquisition</td>\n",
       "      <td>www.smsassist.com</td>\n",
       "      <td>True</td>\n",
       "      <td>sms_assist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  company_id             companies company_former_name  \\\n",
       "0   55185-04              Estimize                 NaN   \n",
       "1   56288-62        New Constructs                 NaN   \n",
       "3   53739-01  Procore Technologies                 NaN   \n",
       "5  153145-27                 Proof   16 Pins, Notarize   \n",
       "6   52304-77            SMS Assist                 NaN   \n",
       "\n",
       "           company_legal_name  \\\n",
       "0              Estimize, Inc.   \n",
       "1         New Constructs, LLC   \n",
       "3  Procore Technologies, Inc.   \n",
       "5              Notarize, Inc.   \n",
       "6          SMS Assist, L.L.C.   \n",
       "\n",
       "                                         competitors  \\\n",
       "0  Neudata, SigFig, Motif (Financial Software), Y...   \n",
       "1  Morningstar, CFRA, Finbox (Media and Informati...   \n",
       "3                       Projectmates, eBuilder, CMiC   \n",
       "5  Templafy, ZorroSign, eOriginal, PandaDoc, Cong...   \n",
       "6  ServiceChannel, Divisions Maintenance Group, T...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Developer of an open financial estimates platf...   \n",
       "1  Operator of an investment research firm intend...   \n",
       "3  Procore Technologies Inc is a cloud-based cons...   \n",
       "5  Developer of an identity-assured transaction m...   \n",
       "6  Provider of business services intended to deli...   \n",
       "\n",
       "                primary_industry_sector primary_industry_group  \\\n",
       "0                Information Technology               Software   \n",
       "1                Information Technology               Software   \n",
       "3                Information Technology               Software   \n",
       "5                Information Technology               Software   \n",
       "6  Business Products and Services (B2B)    Commercial Services   \n",
       "\n",
       "            primary_industry_code  \\\n",
       "0              Financial Software   \n",
       "1              Financial Software   \n",
       "3  Business/Productivity Software   \n",
       "5  Business/Productivity Software   \n",
       "6       Other Commercial Services   \n",
       "\n",
       "                                      all_industries  ...  \\\n",
       "0  Financial Software*, Media and Information Ser...  ...   \n",
       "1  Financial Software*, Media and Information Ser...  ...   \n",
       "3  Business/Productivity Software*, Construction ...  ...   \n",
       "5  Business/Productivity Software*, Media and Inf...  ...   \n",
       "6  Buildings and Property, Business/Productivity ...  ...   \n",
       "\n",
       "  first_financing_valuation first_financing_valuation_status  \\\n",
       "0                      6.34                           Actual   \n",
       "1                      2.17                           Actual   \n",
       "3                      4.00                           Actual   \n",
       "5                     46.50                           Actual   \n",
       "6                       NaN                              NaN   \n",
       "\n",
       "  last_financing_valuation  last_financing_valuation_status  \\\n",
       "0                      NaN                              NaN   \n",
       "1                      NaN                              NaN   \n",
       "3                  8585.03                        Estimated   \n",
       "5                      NaN                              NaN   \n",
       "6                   950.00                        Estimated   \n",
       "\n",
       "  last_known_valuation last_known_valuation_date  \\\n",
       "0                36.00                16/07/2015   \n",
       "1                 2.17                13/05/2003   \n",
       "3              8585.03                20/05/2021   \n",
       "5               760.00                25/03/2021   \n",
       "6               950.00                05/01/2023   \n",
       "\n",
       "  last_known_valuation_deal_type          processed_url  is_accessible  \\\n",
       "0                 Early Stage VC       www.estimize.com           True   \n",
       "1                 Early Stage VC  www.newconstructs.com           True   \n",
       "3                            IPO        www.procore.com           True   \n",
       "5                 Later Stage VC          www.proof.com           True   \n",
       "6             Merger/Acquisition      www.smsassist.com           True   \n",
       "\n",
       "         processed_name  \n",
       "0              estimize  \n",
       "1        new_constructs  \n",
       "3  procore_technologies  \n",
       "5                 proof  \n",
       "6            sms_assist  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('data/PitchBook_All_Columns_2024_07_04_14_48_36_accessibility.csv')\n",
    "df_all = df_all[~df_all['business_status'].isin(['Out of Business', 'Bankruptcy: Liquidation', 'Bankruptcy: Admin/Reorg'])]\n",
    "df_all['companies'] = df_all['companies'].str.replace(r'\\s*\\(.*?\\)\\s*', '', regex=True)\n",
    "df_all = df_all[df_all['is_accessible'] == True]\n",
    "df_all['processed_name'] = df_all['companies'].apply(process_company_name)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_info(processed_name:str, column_name:str):\n",
    "    df_all = pd.read_csv('data/PitchBook_All_Columns_2024_07_04_14_48_36_accessibility.csv')\n",
    "    df_all['companies'] = df_all['companies'].str.replace(r'\\s*\\(.*?\\)\\s*', '', regex=True)\n",
    "    df_all['processed_name'] = df_all['companies'].apply(process_company_name)\n",
    "    \n",
    "    df_select = df_all[df_all['processed_name'] == processed_name]\n",
    "    if len(df_select) > 0:\n",
    "        return df_select[column_name].iloc[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('companies_urls_info.csv')\n",
    "sample = df[~df['url'].isin(['https://www.vertice.one', \n",
    "                    'https://www.estimize.com',\n",
    "                    'https://www.newconstructs.com',\n",
    "                    'https://www.chargebee.com',\n",
    "                    'https://www.bennie.com',\n",
    "                    'https://www.aercompliance.com',\n",
    "                    'https://www.missionmark.com',\n",
    "                    'https://www.joinmassive.com',\n",
    "                    'https://www.hemlane.com',\n",
    "                    'https://www.vesta.com',\n",
    "                    'https://www.adaptive.build',\n",
    "                    'https://www.additive.ai',\n",
    "                    'https://www.9fin.com',\n",
    "                    'https://www.niloom.ai',\n",
    "                    'https://www.nexben.com',\n",
    "                    'https://www.naturealpha.ai',\n",
    "                    'https://www.lworks.io',\n",
    "                    'https://www.infogrid.io',\n",
    "                    'https://www.harnessproperty.com',\n",
    "                    'https://www.directsoftware.com',\n",
    "                    'https://www.dexitcorp.com',\n",
    "                    'https://www.bankerslab.com',\n",
    "                    'https://www.avyst.com',\n",
    "                    'https://www.aggregion.com',\n",
    "                    'https://www.validifi.com',\n",
    "                    'https://www.revvin.com',\n",
    "                    'https://www.gotyou.co',\n",
    "                    'https://www.credenza3.com',\n",
    "                    'https://www.concertocard.com',\n",
    "                    'https://www.element.io',\n",
    "                    'https://www.deepview.com',\n",
    "                    'https://www.realgrader.com',\n",
    "                    'https://www.fanpage.com',\n",
    "                    'https://www.insurgrid.com',\n",
    "                    'https://www.cobalt.pe',\n",
    "                    'https://www.soundout.com',\n",
    "                    'https://www.imoto.com',\n",
    "                    'https://www.ontheupper.com',\n",
    "                    'https://www.getzorba.com',\n",
    "                    'https://www.paralian.io',\n",
    "                    'https://www.gzi.finance',\n",
    "                    'https://www.retailmarketpoint.com',\n",
    "                    'https://www.yardikube.com',\n",
    "                    'https://www.getwats.com',\n",
    "                    'https://www.truelytics.com',\n",
    "                    'https://www.trykintsugi.com',\n",
    "                    'https://www.veruna.com',\n",
    "                    'https://www.tailpath.com',\n",
    "                    'https://www.worksmith.com',\n",
    "                    'https://www.go-maestro.com',\n",
    "                    'https://www.goblueswipe.com',\n",
    "                    'https://www.useink.com',\n",
    "                    'https://www.verdata.com',\n",
    "                    'https://www.beauhurst.com',\n",
    "                    'https://www.saltmine.com',\n",
    "                    'https://www.nammu21.com',\n",
    "                    'https://www.alaffiahealth.com',\n",
    "                    'https://www.bookingpal.com',\n",
    "                    'https://www.metrika.co',\n",
    "                    'https://www.accumula.com',\n",
    "                    'https://www.flowfi.com',\n",
    "                    'https://www.prodeal360.com',\n",
    "                    'https://www.krowdit.com',\n",
    "                    'https://www.jibtechnologies.com',\n",
    "                    'https://www.fieldwire.com',\n",
    "                    'https://www.commercesync.com',\n",
    "                    'https://www.arcana.io',\n",
    "                    'https://www.copernicspace.com',\n",
    "                    'https://www.youattest.com',\n",
    "                    'https://www.pilotbird.com',\n",
    "                    'https://www.pocketbook.tech',\n",
    "                    'https://www.chainlinklabs.com',\n",
    "                    'https://www.proper.ai',\n",
    "                    'https://www.layer.team',\n",
    "                    'https://www.veriphyanalytics.com',\n",
    "                    'https://www.wearegroov.io',\n",
    "                    'https://www.buildstock.com',\n",
    "                    'https://www.scriptainsights.com',\n",
    "                    'https://www.solisolutions.net',\n",
    "                    'https://www.titanpay.ai',\n",
    "                    'https://www.herondata.io',\n",
    "                    'https://www.locatestrategy.com',\n",
    "                    'https://www.hopemacy.com',\n",
    "                    'https://www.smartpayllc.com',\n",
    "                    'https://www.prospectnow.com',\n",
    "                    'https://www.hashku.com',\n",
    "                    'https://www.prismdata.com',\n",
    "                    'https://www.taxometry.com',\n",
    "                    'https://www.r3vl.xyz',\n",
    "                    'https://www.avantarisk.com',\n",
    "                    'https://www.every.io',\n",
    "                    'https://www.joot.io',\n",
    "                    'https://www.buildops.com',\n",
    "                    'https://www.downtobid.com',\n",
    "                    'https://www.plural.ai',\n",
    "                    'https://www.bitwage.com',\n",
    "                    'https://www.gorodeo.app',\n",
    "                    'https://www.ledgible.io',\n",
    "                    'https://www.artd.ai',\n",
    "                    'https://www.acumatica.com',\n",
    "                    'https://www.carby.cc',\n",
    "                    'https://www.shibuya.film',\n",
    "                    'https://www.trelora.com',\n",
    "                    'https://www.regeo.co',\n",
    "                    'https://www.sustainround.com',\n",
    "                    'https://www.cherre.com',\n",
    "                    'https://www.yottled.com',\n",
    "                    'https://www.singularities.com',\n",
    "                    'https://www.domoticsre.com',\n",
    "                    'https://www.dexfreight.io',\n",
    "                    'https://www.nue.io',\n",
    "                    'https://www.atto.co '\n",
    "                    ])]\n",
    "\n",
    "sample = sample.iloc[:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>processed_name</th>\n",
       "      <th>url</th>\n",
       "      <th>related_urls_str</th>\n",
       "      <th>related_urls</th>\n",
       "      <th>num_of_related_urls</th>\n",
       "      <th>all_urls</th>\n",
       "      <th>num_of_all_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Solidspac3</td>\n",
       "      <td>solidspac3</td>\n",
       "      <td>https://www.solidspac3.com</td>\n",
       "      <td>https://www.solidspac3.com</td>\n",
       "      <td>['https://www.solidspac3.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.solidspac3.com', 'https://www.so...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Gravity Software</td>\n",
       "      <td>gravity_software</td>\n",
       "      <td>https://www.gogravity.com</td>\n",
       "      <td>https://www.gogravity.com/product/functionalit...</td>\n",
       "      <td>['https://www.gogravity.com/product/functional...</td>\n",
       "      <td>44</td>\n",
       "      <td>['https://www.gogravity.com/industries/investm...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>IPGen</td>\n",
       "      <td>ipgen</td>\n",
       "      <td>https://www.ipgen.io</td>\n",
       "      <td>https://www.ipgen.io</td>\n",
       "      <td>['https://www.ipgen.io']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.ipgen.io/law-firms/', 'https://w...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>The Booking Factory</td>\n",
       "      <td>the_booking_factory</td>\n",
       "      <td>https://www.thebookingfactory.com</td>\n",
       "      <td>https://www.thebookingfactory.com/services#par...</td>\n",
       "      <td>['https://www.thebookingfactory.com/services#p...</td>\n",
       "      <td>10</td>\n",
       "      <td>['https://www.thebookingfactory.com/white-labe...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Combat IQ</td>\n",
       "      <td>combat_iq</td>\n",
       "      <td>https://www.combatiq.io</td>\n",
       "      <td>https://www.combatiq.io</td>\n",
       "      <td>['https://www.combatiq.io']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.combatiq.io/schedule-demo', 'htt...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Anybill</td>\n",
       "      <td>anybill</td>\n",
       "      <td>https://www.anybill.com</td>\n",
       "      <td>https://www.anybill.com/services,https://www.a...</td>\n",
       "      <td>['https://www.anybill.com/services', 'https://...</td>\n",
       "      <td>4</td>\n",
       "      <td>['https://www.anybill.com/careers', 'https://w...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Caligotech</td>\n",
       "      <td>caligotech</td>\n",
       "      <td>https://www.caligotech.com</td>\n",
       "      <td>https://www.caligotech.com</td>\n",
       "      <td>['https://www.caligotech.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.caligotech.com/careers', 'https:...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>iLumen</td>\n",
       "      <td>ilumen</td>\n",
       "      <td>https://www.ilumen.com</td>\n",
       "      <td>https://www.ilumen.com/case-studies,https://ww...</td>\n",
       "      <td>['https://www.ilumen.com/case-studies', 'https...</td>\n",
       "      <td>5</td>\n",
       "      <td>['https://www.ilumen.com/case-studies', 'https...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>iknowa</td>\n",
       "      <td>iknowa</td>\n",
       "      <td>https://www.iknowa.com</td>\n",
       "      <td>https://www.iknowa.com</td>\n",
       "      <td>['https://www.iknowa.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.iknowa.com/', 'https://www.iknow...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Loanbase</td>\n",
       "      <td>loanbase</td>\n",
       "      <td>https://www.loanbase.com</td>\n",
       "      <td>https://www.loanbase.com/case-studies/,https:/...</td>\n",
       "      <td>['https://www.loanbase.com/case-studies/', 'ht...</td>\n",
       "      <td>2</td>\n",
       "      <td>['https://www.loanbase.com#brokers', 'https://...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 company       processed_name  \\\n",
       "269           Solidspac3           solidspac3   \n",
       "270     Gravity Software     gravity_software   \n",
       "271                IPGen                ipgen   \n",
       "272  The Booking Factory  the_booking_factory   \n",
       "273            Combat IQ            combat_iq   \n",
       "274              Anybill              anybill   \n",
       "275           Caligotech           caligotech   \n",
       "276               iLumen               ilumen   \n",
       "277               iknowa               iknowa   \n",
       "278             Loanbase             loanbase   \n",
       "\n",
       "                                   url  \\\n",
       "269         https://www.solidspac3.com   \n",
       "270          https://www.gogravity.com   \n",
       "271               https://www.ipgen.io   \n",
       "272  https://www.thebookingfactory.com   \n",
       "273            https://www.combatiq.io   \n",
       "274            https://www.anybill.com   \n",
       "275         https://www.caligotech.com   \n",
       "276             https://www.ilumen.com   \n",
       "277             https://www.iknowa.com   \n",
       "278           https://www.loanbase.com   \n",
       "\n",
       "                                      related_urls_str  \\\n",
       "269                         https://www.solidspac3.com   \n",
       "270  https://www.gogravity.com/product/functionalit...   \n",
       "271                               https://www.ipgen.io   \n",
       "272  https://www.thebookingfactory.com/services#par...   \n",
       "273                            https://www.combatiq.io   \n",
       "274  https://www.anybill.com/services,https://www.a...   \n",
       "275                         https://www.caligotech.com   \n",
       "276  https://www.ilumen.com/case-studies,https://ww...   \n",
       "277                             https://www.iknowa.com   \n",
       "278  https://www.loanbase.com/case-studies/,https:/...   \n",
       "\n",
       "                                          related_urls  num_of_related_urls  \\\n",
       "269                     ['https://www.solidspac3.com']                    1   \n",
       "270  ['https://www.gogravity.com/product/functional...                   44   \n",
       "271                           ['https://www.ipgen.io']                    1   \n",
       "272  ['https://www.thebookingfactory.com/services#p...                   10   \n",
       "273                        ['https://www.combatiq.io']                    1   \n",
       "274  ['https://www.anybill.com/services', 'https://...                    4   \n",
       "275                     ['https://www.caligotech.com']                    1   \n",
       "276  ['https://www.ilumen.com/case-studies', 'https...                    5   \n",
       "277                         ['https://www.iknowa.com']                    1   \n",
       "278  ['https://www.loanbase.com/case-studies/', 'ht...                    2   \n",
       "\n",
       "                                              all_urls  num_of_all_urls  \n",
       "269  ['https://www.solidspac3.com', 'https://www.so...                2  \n",
       "270  ['https://www.gogravity.com/industries/investm...               77  \n",
       "271  ['https://www.ipgen.io/law-firms/', 'https://w...               18  \n",
       "272  ['https://www.thebookingfactory.com/white-labe...               32  \n",
       "273  ['https://www.combatiq.io/schedule-demo', 'htt...                7  \n",
       "274  ['https://www.anybill.com/careers', 'https://w...               12  \n",
       "275  ['https://www.caligotech.com/careers', 'https:...                9  \n",
       "276  ['https://www.ilumen.com/case-studies', 'https...               19  \n",
       "277  ['https://www.iknowa.com/', 'https://www.iknow...                2  \n",
       "278  ['https://www.loanbase.com#brokers', 'https://...               16  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>processed_name</th>\n",
       "      <th>url</th>\n",
       "      <th>related_urls_str</th>\n",
       "      <th>related_urls</th>\n",
       "      <th>num_of_related_urls</th>\n",
       "      <th>all_urls</th>\n",
       "      <th>num_of_all_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Rental Beast</td>\n",
       "      <td>rental_beast</td>\n",
       "      <td>https://www.rentalbeast.com</td>\n",
       "      <td>https://www.rentalbeast.com</td>\n",
       "      <td>['https://www.rentalbeast.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.rentalbeast.com/about-rental-bea...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Nophin</td>\n",
       "      <td>nophin</td>\n",
       "      <td>https://www.nophin.com</td>\n",
       "      <td>https://www.nophin.com</td>\n",
       "      <td>['https://www.nophin.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.nophin.com/terms-of-use', 'https...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Candidly</td>\n",
       "      <td>candidly</td>\n",
       "      <td>https://www.getcandidly.com</td>\n",
       "      <td>https://www.getcandidly.com</td>\n",
       "      <td>['https://www.getcandidly.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.getcandidly.com', 'https://www.g...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Mango REIX</td>\n",
       "      <td>mango_reix</td>\n",
       "      <td>https://www.mangoreix.com</td>\n",
       "      <td>https://www.mangoreix.com</td>\n",
       "      <td>['https://www.mangoreix.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.mangoreix.com/_files/ugd/b3a289_...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Next Quarter</td>\n",
       "      <td>next_quarter</td>\n",
       "      <td>https://www.nextq.ai</td>\n",
       "      <td>https://www.nextq.ai</td>\n",
       "      <td>['https://www.nextq.ai']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.nextq.ai', 'https://www.nextq.ai...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Howsy</td>\n",
       "      <td>howsy</td>\n",
       "      <td>https://www.howsy.com</td>\n",
       "      <td>https://www.howsy.com</td>\n",
       "      <td>['https://www.howsy.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.howsy.com']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Terrene Labs</td>\n",
       "      <td>terrene_labs</td>\n",
       "      <td>https://www.terrenelabs.com</td>\n",
       "      <td>https://www.terrenelabs.com</td>\n",
       "      <td>['https://www.terrenelabs.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.terrenelabs.com']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>AuthorLoyalty</td>\n",
       "      <td>authorloyalty</td>\n",
       "      <td>https://www.authorloyalty.com</td>\n",
       "      <td>https://www.authorloyalty.com</td>\n",
       "      <td>['https://www.authorloyalty.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.authorloyalty.com']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Solidspac3</td>\n",
       "      <td>solidspac3</td>\n",
       "      <td>https://www.solidspac3.com</td>\n",
       "      <td>https://www.solidspac3.com</td>\n",
       "      <td>['https://www.solidspac3.com']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.solidspac3.com', 'https://www.so...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>IPGen</td>\n",
       "      <td>ipgen</td>\n",
       "      <td>https://www.ipgen.io</td>\n",
       "      <td>https://www.ipgen.io</td>\n",
       "      <td>['https://www.ipgen.io']</td>\n",
       "      <td>1</td>\n",
       "      <td>['https://www.ipgen.io/law-firms/', 'https://w...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           company processed_name                            url  \\\n",
       "91    Rental Beast   rental_beast    https://www.rentalbeast.com   \n",
       "92          Nophin         nophin         https://www.nophin.com   \n",
       "93        Candidly       candidly    https://www.getcandidly.com   \n",
       "96      Mango REIX     mango_reix      https://www.mangoreix.com   \n",
       "98    Next Quarter   next_quarter           https://www.nextq.ai   \n",
       "..             ...            ...                            ...   \n",
       "256          Howsy          howsy          https://www.howsy.com   \n",
       "258   Terrene Labs   terrene_labs    https://www.terrenelabs.com   \n",
       "262  AuthorLoyalty  authorloyalty  https://www.authorloyalty.com   \n",
       "269     Solidspac3     solidspac3     https://www.solidspac3.com   \n",
       "271          IPGen          ipgen           https://www.ipgen.io   \n",
       "\n",
       "                  related_urls_str                       related_urls  \\\n",
       "91     https://www.rentalbeast.com    ['https://www.rentalbeast.com']   \n",
       "92          https://www.nophin.com         ['https://www.nophin.com']   \n",
       "93     https://www.getcandidly.com    ['https://www.getcandidly.com']   \n",
       "96       https://www.mangoreix.com      ['https://www.mangoreix.com']   \n",
       "98            https://www.nextq.ai           ['https://www.nextq.ai']   \n",
       "..                             ...                                ...   \n",
       "256          https://www.howsy.com          ['https://www.howsy.com']   \n",
       "258    https://www.terrenelabs.com    ['https://www.terrenelabs.com']   \n",
       "262  https://www.authorloyalty.com  ['https://www.authorloyalty.com']   \n",
       "269     https://www.solidspac3.com     ['https://www.solidspac3.com']   \n",
       "271           https://www.ipgen.io           ['https://www.ipgen.io']   \n",
       "\n",
       "     num_of_related_urls                                           all_urls  \\\n",
       "91                     1  ['https://www.rentalbeast.com/about-rental-bea...   \n",
       "92                     1  ['https://www.nophin.com/terms-of-use', 'https...   \n",
       "93                     1  ['https://www.getcandidly.com', 'https://www.g...   \n",
       "96                     1  ['https://www.mangoreix.com/_files/ugd/b3a289_...   \n",
       "98                     1  ['https://www.nextq.ai', 'https://www.nextq.ai...   \n",
       "..                   ...                                                ...   \n",
       "256                    1                          ['https://www.howsy.com']   \n",
       "258                    1                    ['https://www.terrenelabs.com']   \n",
       "262                    1                  ['https://www.authorloyalty.com']   \n",
       "269                    1  ['https://www.solidspac3.com', 'https://www.so...   \n",
       "271                    1  ['https://www.ipgen.io/law-firms/', 'https://w...   \n",
       "\n",
       "     num_of_all_urls  \n",
       "91                25  \n",
       "92                 4  \n",
       "93                 2  \n",
       "96                 3  \n",
       "98                 5  \n",
       "..               ...  \n",
       "256                1  \n",
       "258                1  \n",
       "262                1  \n",
       "269                2  \n",
       "271               18  \n",
       "\n",
       "[87 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample['num_of_related_urls']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurs on .DS_Store: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "doc_list = os.listdir('scraping_output_v2_raw')\n",
    "for doc in doc_list:\n",
    "    \n",
    "    try: \n",
    "        data = read_json_file(f'scraping_output_v2_raw/{doc}')\n",
    "        if 'timestamp' not in data:\n",
    "            current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "            data['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\")\n",
    "            \n",
    "        if \"processed_company\" not in data:\n",
    "            process_name = doc.replace('.json', '')\n",
    "            data[\"processed_company\"] = process_name\n",
    "            \n",
    "        if \"url\" not in data:\n",
    "            data[\"url\"] = \"https://\" + get_additional_info(process_name, 'processed_url')\n",
    "        \n",
    "        write_json_file(f'scraping_output_v2_raw/{doc}', data)\n",
    "    except Exception as e:\n",
    "        print(f'Error occurs on {doc}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Scraping data\", position=0, leave=True):\n",
    "    base_url = row['url']\n",
    "    url_list = ast.literal_eval(row['related_urls'])\n",
    "    result = crawl_data(base_url, url_list, f'scraping_output_v2_raw/{row[\"processed_name\"]}.json', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/realtor-partners/\n",
      "Estimated GPT4-o cost: $0.08219499999999999\n",
      "Estimated GPT4-o cost after cleaning: $0.00167\n",
      "------------------------\n",
      "main_page\n",
      "Estimated GPT4-o cost: $0.01288\n",
      "Estimated GPT4-o cost after cleaning: $0.005535\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for url, content in data.items():\n",
    "    print(url)\n",
    "    print(f'Estimated GPT4-o cost: ${calculate_cost(data[url])}')\n",
    "    print(f'Estimated GPT4-o cost after cleaning: ${calculate_cost(clean_scraped_content(data[url]))}')\n",
    "    print('------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of first shorten the page by extracting relevant information\n",
    "Issue: The output of the content might be shorten too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def llm_summary(text, model_name=\"gpt-4o\"):\n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to extract information \n",
    "    from the given text and convert it into a text (string) format. \n",
    "    The output response should contain only the data extracted from the text, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing. Do not hallucinate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the extraction prompt\n",
    "    extraction_prompt = \"\"\"\n",
    "    You are provided with a text obtained from a company's webpage. Your task is to extract any sections or paragraphs that are relevant to the specified information of interest.\n",
    "\n",
    "    ## Information of Interest:\n",
    "\n",
    "    1. **About Product or Service**:\n",
    "    - Any details about the products or services the company offers, including their features.\n",
    "\n",
    "    2. **About Partner or Client**:\n",
    "    - Any information about the company's partners or clients.\n",
    "    - Any use cases (case studies) describing how a client is using the company's product or service.\n",
    "    \n",
    "    ## Note:\n",
    "    Sometimes, the company does not explicit describe their clients and the client use case, instead, they will only display clients' logos. \n",
    "    You then need to extract client's name from their logos. \n",
    "    \n",
    "    ## Instructions:\n",
    "    - Do not summarize the content. Extract the raw lines or sections as they are.\n",
    "    - If you are unsure about the relevance of the information, include it to ensure comprehensive coverage.\n",
    "    - Output the extracted information in standard text format.\n",
    "\n",
    "    ## Examples:\n",
    "\n",
    "    ### Example 1: Product or Service\n",
    "    If the input text contains:\n",
    "    \"Our company offers innovative cloud solutions that help businesses streamline their operations. Our key features include scalability, security, and ease of use.\n",
    "    We partner with leading firms such as TechCorp and SoftInc to deliver top-notch services.\"\n",
    "\n",
    "    The output should be:\n",
    "    \"Our company offers innovative cloud solutions that help businesses streamline their operations. Our key features include scalability, security, and ease of use.\n",
    "    We partner with leading firms such as TechCorp and SoftInc to deliver top-notch services.\"\n",
    "\n",
    "    ### Example 2: Client Logos\n",
    "    If the input text contains:\n",
    "    \"Our platform and service is trusted by these innovative companies:\n",
    "    ![Nationwide Logo]\n",
    "    ![Freedom 365 Logo]\n",
    "    ![Bestow Logo]\n",
    "    ...\"\n",
    "    \n",
    "    The output should be:\n",
    "    \"Our platform and service is trusted by these innovative companies: \n",
    "    Clients are: Nationwide, Freedom 365, Bestow...\"\n",
    "   \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message),\n",
    "            (\"system\", extraction_prompt),\n",
    "            (\"human\", \"Use the given text to extract information: {input}\"),\n",
    "            (\"human\", \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                ## Rules:\n",
    "                - Make sure to answer in the standard text format.\n",
    "                - If no information is provided, return nothing.\n",
    "                - DO NOT HALLUCINATE.\n",
    "             \"\"\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(openai_api_key=os.getenv('OPENAI_KEY'),\n",
    "                    temperature=0, \n",
    "                    model_name=model_name)\n",
    "\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = llm_chain.invoke({'input': text})\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_summary_execution(processed_name:str, \n",
    "                          scrape_file_path:str,\n",
    "                          summary_file_path:str,\n",
    "                          overwrite:bool = False, \n",
    "                          model_name:str = 'gpt-4o-mini'):\n",
    "\n",
    "    scrape_data = read_json_file(scrape_file_path)\n",
    "    \n",
    "    file_modified = False\n",
    "\n",
    "    # Load existing data if the file exists\n",
    "    if os.path.exists(summary_file_path):\n",
    "        with open(summary_file_path, 'r') as file:\n",
    "            extracted_data = json.load(file)\n",
    "    else:\n",
    "        extracted_data = {}\n",
    "\n",
    "    for endpoint, content in tqdm(scrape_data.items(), total=len(scrape_data), desc=\"Extracting data\", position=0, leave=True):\n",
    "        if endpoint in ['timestamp', 'processed_company', 'url']:\n",
    "            continue\n",
    "        if endpoint in extracted_data and not overwrite:\n",
    "            print(f\"Company: {processed_name}; Skipping {endpoint} as it already exists and overwrite is set to False.\")\n",
    "            continue  # Skip this URL and move to the next one\n",
    "        else:\n",
    "            clean_content = clean_scraped_content(content)\n",
    "            extracted_data[endpoint] = llm_summary(text = clean_content, model_name = model_name)\n",
    "            print(f'Company: {processed_name}; Content in {endpoint} is extracted.')\n",
    "            \n",
    "            current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "            extracted_data['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\") + ' Etc/GMT'\n",
    "            file_modified = True\n",
    "    \n",
    "    if file_modified:\n",
    "        extracted_data['processed_company'] = processed_name\n",
    "        extracted_data['url'] = \"https://\" + get_additional_info(processed_name, 'processed_url')\n",
    "        write_json_file(summary_file_path, extracted_data)\n",
    "        \n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  31%|███       | 4/13 [00:09<00:22,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#partner-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  38%|███▊      | 5/13 [00:15<00:26,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#rev-plus is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  46%|████▌     | 6/13 [00:20<00:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /customer-agreement is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  54%|█████▍    | 7/13 [00:26<00:26,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#hotel-it is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  62%|██████▏   | 8/13 [00:32<00:24,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  69%|██████▉   | 9/13 [00:39<00:21,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#accounting-services is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  77%|███████▋  | 10/13 [00:45<00:16,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#basic-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  85%|████████▍ | 11/13 [00:50<00:11,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /partner-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  92%|█████████▏| 12/13 [00:58<00:06,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in main_page is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|██████████| 13/13 [01:04<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#bf-web is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_name = 'the_booking_factory'\n",
    "scrape_file_path = f'scraping_output_v2_raw/{processed_name}.json'\n",
    "summary_file_path = f'extraction_summary_v2/{processed_name}_summary_str.json'\n",
    "\n",
    "response = llm_summary_execution(processed_name = processed_name,\n",
    "                                 scrape_file_path = scrape_file_path,\n",
    "                                 summary_file_path = summary_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor\n",
    "\n",
    "https://github.com/jxnl/instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "class ProductDescription(BaseModel):\n",
    "    name: str = Field(..., alias='summarised name of product')\n",
    "    description: str = Field(..., alias='concise features description of the product or service')\n",
    "\n",
    "class SummaryProductDescription(BaseModel):\n",
    "    name: str = Field(..., alias='summarised name of the main product offerings of the company')\n",
    "    description: str = Field(..., alias='summary of product offering of the company')\n",
    "\n",
    "class ClientDescription(BaseModel):\n",
    "    name: str = Field(..., alias='name of the client or partner')\n",
    "    description: Optional[str] = Field(None, alias='description of the usecase')\n",
    "\n",
    "class ExtractedInformation(BaseModel):\n",
    "    product_descriptions: Optional[List[ProductDescription]] = None\n",
    "    summary_product_description: Optional[SummaryProductDescription] = None\n",
    "    client_descriptions: Optional[List[ClientDescription]] = None\n",
    "    \n",
    "class ValidatedClientDescription(BaseModel):\n",
    "    name: str = Field(..., alias='name of the client or partner')\n",
    "    entity_type: Literal[\"person\", \"company\", \"general_entity\", \"other\", \"school\"]\n",
    "    product_used: Optional[str] = Field(None, alias='summary of the product or service used by the client or partner')\n",
    "    description: Optional[str] = Field(None, alias='description of the usecase')\n",
    "\n",
    "class ValidatedExtractedInformation(BaseModel):\n",
    "    # product_descriptions: Optional[List[ProductDescription]] = None\n",
    "    # summary_product_description: Optional[SummaryProductDescription] = None\n",
    "    client_descriptions: Optional[List[ValidatedClientDescription]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def initial_extraction(text: str, model_name: str = 'gpt-4o', additional_context: str = None) -> ExtractedInformation:\n",
    "    \n",
    "    # Patch the OpenAI client with Instructor\n",
    "    client = instructor.from_openai(OpenAI(api_key=os.getenv('OPENAI_KEY')))\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to extract structured information \n",
    "    from the given text and convert it into a structured format. \n",
    "    The output response should contain only the data extracted from the text, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing for that field. Do not hallucinate.\n",
    "    \"\"\"\n",
    "    \n",
    "    custom_extraction_prompt = \"\"\"\n",
    "    Extract the following information from the text extracted from a webpage of a company:\n",
    "\n",
    "    1. Product Description:\n",
    "    - What service or product does the company provide?\n",
    "    - What features does the product or service have?\n",
    "    Note: If the company has more than one product or service, automatically detect and list each product with its relevant details.\n",
    "    \n",
    "    2. Summary of Product Offering:\n",
    "    - Summary of the description of the service that the company provide, taking into consideration of all the product offerings.\n",
    "    Note: Do not include any company-specific information in the summary, such as company name and location.\n",
    "    \n",
    "    3. Client Description:\n",
    "    - Name of the corporate client or partner. \n",
    "    - Description of the use case.\n",
    "    Note: Focus on the extraction of company's name, instead of individuals.\n",
    "    Note: If the description of the use case is not mentioned, it should be None.\n",
    "    \n",
    "\n",
    "    Output in a structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    rule_prompt = \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                    ## Rules:\n",
    "                    - The aim is to achieve simplicity and clarity in the extracted text.\n",
    "                    - Make sure to answer in the structured format.\n",
    "                    - If no information is provided for any of the fields, return nothing of that field.\n",
    "                    - DO NOT HALLUCINATE.\n",
    "                \"\"\"\n",
    "    \n",
    "    extraction_prompt = f\"\"\"\n",
    "    {system_message}\n",
    "    {custom_extraction_prompt}\n",
    "    \"\"\"\n",
    "    \n",
    "    if additional_context:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            response_model=ExtractedInformation,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": extraction_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Use the given text to extract information: {text}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Here are some additional descriptions about this company for your reference:\n",
    "                                                {additional_context}\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": rule_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            response_model=ExtractedInformation,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": extraction_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Use the given text to extract information: {text}\"},\n",
    "                {\"role\": \"user\", \"content\": rule_prompt}\n",
    "            ]\n",
    "        )\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_validation(products: list, clients: list, summary: dict, model_name: str = 'gpt-4o') -> ValidatedExtractedInformation:\n",
    "    \n",
    "    # Patch the OpenAI client with Instructor\n",
    "    client = instructor.from_openai(OpenAI(api_key=os.getenv('OPENAI_KEY')))\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to validate the client information, classify the client names into different entity types, and determine which product is likely used by the client. \n",
    "    The output response should contain only the data validated and assigned, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing for that field. Do not hallucinate.\n",
    "    \"\"\"\n",
    "    \n",
    "    product_info = \"\\n\".join([f\"Product: {p['name']}; Description: {p['description']}\" for p in products])\n",
    "    client_info = \"\\n\".join([f\"Client: {c['name']}; Description: {c['description']}\" for c in clients])\n",
    "    summary_info = f\"{summary['name']}: {summary['description']}\"\n",
    "    \n",
    "    few_shot_examples = \"\"\"\n",
    "        ## Example 1:\n",
    "        Client Name: Mike Johnson, CEO of TechCorp\n",
    "        Entity_type: person\n",
    "        - Reason: Mike Johnson is the name of a person. \n",
    "        \n",
    "        ## Example 2:\n",
    "        Client Name: Government\n",
    "        Entity_type: general_entity\n",
    "        - Reason: \"Government\" is a general entity, not a specific company.\n",
    "\n",
    "        ## Example 3:\n",
    "        Client Name: Innovative Solutions LLC\n",
    "        Entity_type: company\n",
    "        - Reason: Innovative Solutions LLC is a specific company name.\n",
    "        \n",
    "        ## Example 4:\n",
    "        Client Name: A US resort\n",
    "        Entity_type: general_entity\n",
    "        - Reason: \"A US resort\" is a general description, not a specific company name.\n",
    "    \n",
    "        ## Example 5: \n",
    "        Client Name: University College London\n",
    "        Entity_type: school\n",
    "        - Reason: University College London is a specific school name.\n",
    "    \"\"\"\n",
    "\n",
    "    validation_prompt = f\"\"\"\n",
    "    {system_message}\n",
    "    Here is the product information extracted:\n",
    "    {product_info}\n",
    "    \n",
    "    Here is the summary of product offerings of the company:\n",
    "    {summary_info}\n",
    "    \n",
    "    Here are the clients and their use cases:\n",
    "    {client_info}\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Classify each client name into one of the following entity types: person, company, general_entity, school, or other.\n",
    "       Note: the entity type \"company\" should be given to specific companies, with company names.\n",
    "    2. Based on the product descriptions and client use cases, assign the most likely product used by each client. \n",
    "       If you are not confident about which product the client uses, return None for that field.\n",
    "\n",
    "    Here are some examples regarding the classifying clients into different entity types:\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Output in a structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        response_model=ValidatedExtractedInformation,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": validation_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                ## Rules:\n",
    "                - Classify each client name into one of the following entity types: person, company, general_entity, school, or other.\n",
    "                - Assign the most likely product used by each client based on the provided product descriptions and use cases.\n",
    "                - If the product used is not clear, return None for that field.\n",
    "                - Make sure to answer in the structured format.\n",
    "                - DO NOT HALLUCINATE.\n",
    "            \"\"\"},\n",
    "        ]\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_extraction_execution(processed_name:str, \n",
    "                             summary_file_path:str,\n",
    "                             extraction_file_path:str, \n",
    "                             include_additional_context:bool = True, \n",
    "                             overwrite:bool = False):\n",
    "    \n",
    "    if not overwrite and os.path.exists(extraction_file_path):\n",
    "        print(f\"Company: {processed_name}; Skipping extraction as the extraction file already exists and overwrite is set to False.\")\n",
    "        return None\n",
    "    else:\n",
    "        if os.path.exists(summary_file_path):\n",
    "            with open(summary_file_path, 'r') as file:\n",
    "                summary = json.load(file)\n",
    "\n",
    "            combined_summary = f\"## Main Page:\\n {summary['main_page']}\\n----------------\\n\"\n",
    "\n",
    "            for endpoint, text in summary.items():\n",
    "                if endpoint not in [\"main_page\", \"timestamp\", \"processed_company\", \"url\"]:\n",
    "                    combined_summary += f\"## {endpoint}:\\n{text}\\n----------------\\n\"\n",
    "            \n",
    "            print(f\"Company: {processed_name}; Information extraction begins.\")\n",
    "            if include_additional_context:\n",
    "                context = get_additional_info(processed_name, 'description')\n",
    "                \n",
    "                print(f'Company: {processed_name}; Estimated Cost: ${calculate_cost(combined_summary + context)}')\n",
    "                print(f'Company: {processed_name}; Pitchbook description obtained: {context}')\n",
    "                \n",
    "                initial_response = initial_extraction(text = combined_summary, \n",
    "                                                additional_context = context).dict()\n",
    "                \n",
    "            else:\n",
    "                print(f'Company: {processed_name}; Estimated Cost: ${calculate_cost(combined_summary)}')\n",
    "                initial_response = initial_extraction(text = combined_summary, \n",
    "                                            additional_context = None).dict()\n",
    "            \n",
    "            print(f'Company: {processed_name}; PART 1 - Initial extraction is completed.')\n",
    "            \n",
    "            result = initial_response\n",
    "            \n",
    "            if initial_response['client_descriptions']:\n",
    "                products = initial_response['product_descriptions'] if initial_response['product_descriptions'] else []\n",
    "                clients = initial_response['client_descriptions'] if initial_response['client_descriptions'] else []\n",
    "                summary = initial_response['summary_product_description']\n",
    "\n",
    "                validated_response = information_validation(products, clients, summary)\n",
    "                print(f'Company: {processed_name}; PART 2 - Information validation is completed.')\n",
    "                result['validated_client_descriptions'] = validated_response.dict()['client_descriptions']\n",
    "                \n",
    "            else:\n",
    "                print(f'Company: {processed_name}; PART 2 - Skipped, due to lack of client information.')\n",
    "                result['validated_client_descriptions'] = None\n",
    "            \n",
    "            current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "            result['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\") + ' Etc/GMT'\n",
    "            result['processed_company'] = processed_name\n",
    "            result['url'] = \"https://\" + get_additional_info(processed_name, 'processed_url')\n",
    "\n",
    "            write_json_file(extraction_file_path, result)\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(f'Summary file: {summary_file_path} does not exist.')\n",
    "            return None\n",
    "\n",
    "def add_client_url_to_extraction_output(processed_name:str, extraction_file_path:str, verbose:bool = False):\n",
    "    data = read_json_file(extraction_file_path)\n",
    "    \n",
    "    if data['validated_client_descriptions']:\n",
    "        for client in data['validated_client_descriptions']:\n",
    "            if client['entity_type'] != 'company':\n",
    "                client['url'] = None\n",
    "            else:\n",
    "                url = get_and_verify_client_link(client['name'], verbose = verbose)\n",
    "                client['url'] = url\n",
    "        print(f\"Company: {processed_name}; Client is extracted.\")\n",
    "    else:\n",
    "        print(f\"Company: {processed_name}; No clients' information.\")\n",
    "    write_json_file(extraction_file_path, data)    \n",
    "\n",
    "    \n",
    "def get_embedding(text:str, embedding_model:str=\"text-embedding-3-small\"):\n",
    "   client_openai = OpenAI(api_key=os.getenv('OPENAI_KEY'))\n",
    "   \n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client_openai.embeddings.create(input = [text], model=embedding_model).data[0].embedding\n",
    "\n",
    "\n",
    "def get_product_embedding(processed_name:str, extraction_file_path:str, embedding_model:str=\"text-embedding-3-small\"):\n",
    "    \n",
    "    data = read_json_file(extraction_file_path)\n",
    "    # Check wheather embedding has already been done\n",
    "    if 'name_embedding' in data['summary_product_description']:\n",
    "        print(f'Company: {processed_name}; Embedding has already been done.')\n",
    "        pass\n",
    "    else:\n",
    "        product_lst = data['product_descriptions']\n",
    "        for product in product_lst:\n",
    "            product['description_embedding'] = get_embedding(text = product['description'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "            product['name_embedding'] = get_embedding(text = product['name'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "\n",
    "        summary_product = data['summary_product_description']\n",
    "        summary_product['description_embedding'] = get_embedding(text = summary_product['description'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "        summary_product['name_embedding'] = get_embedding(text = summary_product['name'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "        print(f'Company: {processed_name}; Embedding is completed.')\n",
    "        write_json_file(extraction_file_path, data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def update_client_list(processed_name:str, extraction_file_path:str, client_file_path:str = 'data/client_info.json', verbose:bool = False):\n",
    "    \n",
    "    data = read_json_file(extraction_file_path)\n",
    "    client_info = read_json_file(client_file_path)\n",
    "        \n",
    "    if data['validated_client_descriptions']:\n",
    "        try:        \n",
    "            for client in data['validated_client_descriptions']:\n",
    "                if client['entity_type'] != 'company':\n",
    "                    continue\n",
    "                # If a company's name already exists in the dictionary and the url is unchanged\n",
    "                if client['name'] in client_info and client['url'] == client_info[client['name']]['url'] :\n",
    "                    # If its service provider does not appear in the saved list, then append it\n",
    "                    if processed_name not in client_info[client['name']]['service_provider_processed']:\n",
    "                        client_info[client['name']]['service_provider_processed'].append(processed_name)\n",
    "                        client_info[client['name']]['service_provider'].append(get_additional_info(processed_name, 'companies'))\n",
    "                        client_info[client['name']]['service_provider_url'].append('https://' + get_additional_info(processed_name, 'processed_url'))\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f'Company {client[\"name\"]} has already been recorded.')\n",
    "                \n",
    "                # If a company's name already does not exist, add the new company\n",
    "                else:\n",
    "                    client_info[client['name']] = {'processed_name': process_company_name(client['name']),\n",
    "                                        'url': client['url'],\n",
    "                                        'service_provider_processed': [processed_name],\n",
    "                                        'service_provider': [get_additional_info(processed_name, 'companies')],\n",
    "                                        'service_provider_url': ['https://' + get_additional_info(processed_name, 'processed_url')]\n",
    "                                        }\n",
    "            print(f\"Company: {data['processed_company']}; Clients information is updated.\")\n",
    "            write_json_file(client_file_path, client_info)\n",
    "        except Exception as e:\n",
    "            print(f'Company: {processed_name}; Error occurred: {e}')\n",
    "    else:\n",
    "        print(f'Company: {processed_name}; No clients to be updated')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Information extraction begins.\n",
      "Company: the_booking_factory; Estimated Cost: $0.024290000000000003\n",
      "Company: the_booking_factory; Pitchbook description obtained: Developer of a property management platform designed to advertise and conduct hotel operations. The company's platform helps businesses to advertise their hotel rooms for bookings, develop websites for customers with a friendly user interface as well as systems to manage hotel properties, enabling large resorts and small hotel owners to manage and improve their hotel business in an efficient manner.\n",
      "Company: the_booking_factory; PART 1 - Initial extraction is completed.\n",
      "Company: the_booking_factory; PART 2 - Information validation is completed.\n",
      "Company: the_booking_factory; Client is extracted.\n",
      "Company: the_booking_factory; Embedding is completed.\n",
      "Company: the_booking_factory; Clients information is updated.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_name = 'the_booking_factory'\n",
    "summary_file_path = f'extraction_summary_v2/{processed_name}_summary_str.json'\n",
    "extraction_file_path = f'extraction_output_v2/{processed_name}_extraction.json'\n",
    "\n",
    "llm_extraction_execution(processed_name = processed_name,\n",
    "                         summary_file_path = summary_file_path,\n",
    "                         extraction_file_path = extraction_file_path, \n",
    "                         include_additional_context = True, \n",
    "                         overwrite = False)\n",
    "\n",
    "add_client_url_to_extraction_output(processed_name = processed_name,\n",
    "                                    extraction_file_path = extraction_file_path)\n",
    "\n",
    "get_product_embedding(processed_name = processed_name,\n",
    "                      extraction_file_path = extraction_file_path)\n",
    "\n",
    "update_client_list(processed_name = processed_name,\n",
    "                   extraction_file_path = extraction_file_path,\n",
    "                   client_file_path = 'data/client_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: bankerslab; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: bankerslab; No clients' information.\n",
      "Company: bankerslab; Embedding has already been done.\n",
      "Company: bankerslab; No clients to be updated\n",
      "Company: estimize; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: estimize; Client is extracted.\n",
      "Company: estimize; Embedding has already been done.\n",
      "Company: estimize; Clients information is updated.\n",
      "Company: avyst; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: avyst; Client is extracted.\n",
      "Company: avyst; Embedding has already been done.\n",
      "Company: avyst; Clients information is updated.\n",
      "Company: bennie; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: bennie; Client is extracted.\n",
      "Company: bennie; Embedding has already been done.\n",
      "Company: bennie; Clients information is updated.\n",
      "Company: cherre; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: cherre; No clients' information.\n",
      "Company: cherre; Embedding has already been done.\n",
      "Company: cherre; No clients to be updated\n",
      "Company: vertice; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: vertice; Client is extracted.\n",
      "Company: vertice; Embedding has already been done.\n",
      "Company: vertice; Clients information is updated.\n",
      "Company: hemlane; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: hemlane; Client is extracted.\n",
      "Company: hemlane; Embedding has already been done.\n",
      "Company: hemlane; Clients information is updated.\n",
      "Company: naturealpha; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: naturealpha; Client is extracted.\n",
      "Company: naturealpha; Embedding has already been done.\n",
      "Company: naturealpha; Clients information is updated.\n",
      "Company: nexben; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: nexben; No clients' information.\n",
      "Company: nexben; Embedding has already been done.\n",
      "Company: nexben; No clients to be updated\n",
      "Company: ledger_works; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: ledger_works; Client is extracted.\n",
      "Company: ledger_works; Embedding has already been done.\n",
      "Company: ledger_works; Clients information is updated.\n",
      "Company: saltmine; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: saltmine; Client is extracted.\n",
      "Company: saltmine; Embedding has already been done.\n",
      "Company: saltmine; Clients information is updated.\n",
      "Company: vesta; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: vesta; Client is extracted.\n",
      "Company: vesta; Embedding has already been done.\n",
      "Company: vesta; Clients information is updated.\n",
      "Company: direct; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company Luxury RV Rentals LV has error: list index out of range\n",
      "Company: direct; Client is extracted.\n",
      "Company: direct; Embedding has already been done.\n",
      "Company: direct; Clients information is updated.\n",
      "Company: massive; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: massive; Client is extracted.\n",
      "Company: massive; Embedding has already been done.\n",
      "Company: massive; Clients information is updated.\n",
      "Company: missionmark; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: missionmark; Client is extracted.\n",
      "Company: missionmark; Embedding has already been done.\n",
      "Company: missionmark; Clients information is updated.\n",
      "Company: new_constructs; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: new_constructs; Client is extracted.\n",
      "Company: new_constructs; Embedding has already been done.\n",
      "Company: new_constructs; Clients information is updated.\n",
      "Company: aggregion; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: aggregion; Client is extracted.\n",
      "Company: aggregion; Embedding has already been done.\n",
      "Company: aggregion; Clients information is updated.\n",
      "Company: arcana; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: arcana; Client is extracted.\n",
      "Company: arcana; Embedding has already been done.\n",
      "Company: arcana; Clients information is updated.\n",
      "Company: infogrid; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: infogrid; Client is extracted.\n",
      "Company: infogrid; Embedding has already been done.\n",
      "Company: infogrid; Clients information is updated.\n",
      "Company: additive; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: additive; No clients' information.\n",
      "Company: additive; Embedding has already been done.\n",
      "Company: additive; No clients to be updated\n",
      "Company: 9fin; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: 9fin; Client is extracted.\n",
      "Company: 9fin; Embedding has already been done.\n",
      "Company: 9fin; Clients information is updated.\n",
      "Company: adaptive; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: adaptive; Client is extracted.\n",
      "Company: adaptive; Embedding has already been done.\n",
      "Company: adaptive; Clients information is updated.\n",
      "Company: niloom_ai; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: niloom_ai; No clients' information.\n",
      "Company: niloom_ai; Embedding has already been done.\n",
      "Company: niloom_ai; No clients to be updated\n",
      "Company: aer_compliance; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: aer_compliance; No clients' information.\n",
      "Company: aer_compliance; Embedding has already been done.\n",
      "Company: aer_compliance; No clients to be updated\n",
      "Company: dexit; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: dexit; No clients' information.\n",
      "Company: dexit; Embedding has already been done.\n",
      "Company: dexit; No clients to be updated\n",
      "Company: youattest; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company Riskcop Advisory LLC has error: list index out of range\n",
      "Company: youattest; Client is extracted.\n",
      "Company: youattest; Embedding has already been done.\n",
      "Company: youattest; Clients information is updated.\n",
      "Company: harness_data_intelligence; Skipping extraction as the extraction file already exists and overwrite is set to False.\n",
      "Company: harness_data_intelligence; Client is extracted.\n",
      "Company: harness_data_intelligence; Embedding has already been done.\n",
      "Company: harness_data_intelligence; Clients information is updated.\n"
     ]
    }
   ],
   "source": [
    "doc_list = os.listdir('extraction_summary_v2')\n",
    "for doc in doc_list:\n",
    "    processed_name = doc.replace('_summary_str.json', '')\n",
    "    try:\n",
    "        llm_extraction_execution(processed_name = processed_name, \n",
    "                        include_additional_context = True, \n",
    "                        overwrite = False)\n",
    "        add_client_url_to_extraction_output(processed_name = processed_name)\n",
    "        get_product_embedding(processed_name = processed_name)\n",
    "        update_client_list(processed_name = processed_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error occured on company {processed_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: hemlane; Clients information is updated.\n"
     ]
    }
   ],
   "source": [
    "update_client_list('hemlane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to re-validate the client description:\n",
    "\n",
    "processed_name = 'vertice'\n",
    "    \n",
    "extraction_file_path = f'extraction_output_v2/{processed_name}_extraction.json'\n",
    "initial_response = read_json_file(extraction_file_path)\n",
    "if initial_response['client_descriptions']:\n",
    "    products = initial_response['product_descriptions'] if initial_response['product_descriptions'] else []\n",
    "    clients = initial_response['client_descriptions'] if initial_response['client_descriptions'] else []\n",
    "    summary = initial_response['summary_product_description']\n",
    "\n",
    "    validated_response = information_validation(products, clients, summary)\n",
    "    print(f'Company: {processed_name}; PART 2 - Information validation is completed.')\n",
    "    result['validated_client_descriptions'] = validated_response.dict()['client_descriptions']\n",
    "    \n",
    "else:\n",
    "    print(f'Company: {processed_name}; PART 2 - Skipped, due to lack of client information.')\n",
    "    result['validated_client_descriptions'] = None\n",
    "\n",
    "current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "result['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\") + ' Etc/GMT'\n",
    "result['processed_company'] = processed_name\n",
    "result['url'] = \"https://\" + get_additional_info(processed_name, 'processed_url')\n",
    "write_json_file(extraction_file_path, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_file_path = f'extraction_output_v2/9fin_extraction.json'\n",
    "initial_response = read_json_file(extraction_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product: Data & Analytics Platform; Description: Provides AI-powered financial data and analytics. Features real-time market news, key data on high-yield bonds, deal tracking, financial profiles, predictive analytics, search functionality, and ESG data.\\nProduct: Comparables; Description: Benchmark prior transactions, bonds, loans, or company profiles using over 300 credit metrics.\\nProduct: Earnings; Description: AI transcripts and instant analysis for earnings reports.\\nProduct: Search; Description: Powerful search tool for thousands of documents text-searchable by any keyword or phrase.\\nProduct: ESG; Description: A full suite of Environmental, Social, and Governance data and analysis.\\nProduct: Distressed and Restructuring; Description: Tools to spot undervalued credits and potential future restructurings.\\nProduct: News; Description: Aggregates news from 2,000 sources using AI and delivers it quickly.\\nProduct: Financials; Description: Full financial profiles with 3 statements, KPIs, segment splits, and credit metrics traced to source documents.\\nProduct: Covenants; Description: New deal analysis and covenant comparison tools.\\nProduct: Deal predictions; Description: Predicts refinancing, restructuring, and capital markets activity 12 months in advance.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = initial_response['product_descriptions']\n",
    "product_info = \"\\n\".join([f\"Product: {p['name']}; Description: {p['description']}\" for p in products])\n",
    "\n",
    "product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Data & Analytics Platform',\n",
       "  'description': 'Provides AI-powered financial data and analytics. Features real-time market news, key data on high-yield bonds, deal tracking, financial profiles, predictive analytics, search functionality, and ESG data.'},\n",
       " {'name': 'Comparables',\n",
       "  'description': 'Benchmark prior transactions, bonds, loans, or company profiles using over 300 credit metrics.'},\n",
       " {'name': 'Earnings',\n",
       "  'description': 'AI transcripts and instant analysis for earnings reports.'},\n",
       " {'name': 'Search',\n",
       "  'description': 'Powerful search tool for thousands of documents text-searchable by any keyword or phrase.'},\n",
       " {'name': 'ESG',\n",
       "  'description': 'A full suite of Environmental, Social, and Governance data and analysis.'},\n",
       " {'name': 'Distressed and Restructuring',\n",
       "  'description': 'Tools to spot undervalued credits and potential future restructurings.'},\n",
       " {'name': 'News',\n",
       "  'description': 'Aggregates news from 2,000 sources using AI and delivers it quickly.'},\n",
       " {'name': 'Financials',\n",
       "  'description': 'Full financial profiles with 3 statements, KPIs, segment splits, and credit metrics traced to source documents.'},\n",
       " {'name': 'Covenants',\n",
       "  'description': 'New deal analysis and covenant comparison tools.'},\n",
       " {'name': 'Deal predictions',\n",
       "  'description': 'Predicts refinancing, restructuring, and capital markets activity 12 months in advance.'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 355.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company 9fin's clients are recorded.\n",
      "Company Bloomberg has already been recorded.\n",
      "Company Apex Clearing has already been recorded.\n",
      "Company IEX Cloud has already been recorded.\n",
      "Company WisdomTree has already been recorded.\n",
      "Company Viola Risk Advisors has already been recorded.\n",
      "Company new_constructs's clients are recorded.\n",
      "Company .DS_Store has error: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n",
      "Company Green Way Homes has already been recorded.\n",
      "Company Revent Builds has already been recorded.\n",
      "Company Veldhouse Companies has already been recorded.\n",
      "Company Drake Construction Services has already been recorded.\n",
      "Company Riverside Homes has already been recorded.\n",
      "Company Joseph Design & Build has already been recorded.\n",
      "Company adaptive's clients are recorded.\n",
      "Company HP2 RESIDENTIAL has already been recorded.\n",
      "Company Fathom Realty, LLC has already been recorded.\n",
      "Company B Wright At Home LLC has already been recorded.\n",
      "Company hemlane's clients are recorded.\n",
      "Company niloom_ai has error: 'NoneType' object is not iterable\n",
      "Company Airbnb has already been recorded.\n",
      "Company Thoropass has already been recorded.\n",
      "Company Faherty has already been recorded.\n",
      "Company Formstack has already been recorded.\n",
      "Company GameOn has already been recorded.\n",
      "Company ROKT has already been recorded.\n",
      "Company Nylas has already been recorded.\n",
      "Company Warby Parker has already been recorded.\n",
      "Company Bonusly has already been recorded.\n",
      "Company LTSE has already been recorded.\n",
      "Company Superhuman has already been recorded.\n",
      "Company Pliancy has already been recorded.\n",
      "Company Hi Marley has already been recorded.\n",
      "Company Rippling has already been recorded.\n",
      "Company Crisis Text Line has already been recorded.\n",
      "Company Interpretek has already been recorded.\n",
      "Company bennie's clients are recorded.\n",
      "Company SoftBank Robotics has already been recorded.\n",
      "Company JLL has already been recorded.\n",
      "Company JPC by Samsic has already been recorded.\n",
      "Company infogrid's clients are recorded.\n",
      "Company additive has error: 'NoneType' object is not iterable\n",
      "Company MotorK has already been recorded.\n",
      "Company Ebury has already been recorded.\n",
      "Company ba&sh has already been recorded.\n",
      "Company Lincoln Investment has already been recorded.\n",
      "Company Wallbox has already been recorded.\n",
      "Company Matillion has already been recorded.\n",
      "Company PageUp has already been recorded.\n",
      "Company Omio has already been recorded.\n",
      "Company Coronado has already been recorded.\n",
      "Company Revel has already been recorded.\n",
      "Company Podimo has already been recorded.\n",
      "Company Haiilo has already been recorded.\n",
      "Company Le Collectionist has already been recorded.\n",
      "Company Euronext has already been recorded.\n",
      "Company Choco has already been recorded.\n",
      "Company Futureverse has already been recorded.\n",
      "Company Multiplica has already been recorded.\n",
      "Company Crunch has already been recorded.\n",
      "Company Holded has already been recorded.\n",
      "Company Lighthouse has already been recorded.\n",
      "Company A.Team has already been recorded.\n",
      "Company Encompass has already been recorded.\n",
      "Company vertice's clients are recorded.\n",
      "Company Fortune 500 Fintech Company has already been recorded.\n",
      "Company ERP Platform Company has already been recorded.\n",
      "Company Tech Company has already been recorded.\n",
      "Company Financial Services Company has already been recorded.\n",
      "Company International Banking Company has already been recorded.\n",
      "Company IT Company has already been recorded.\n",
      "Company saltmine's clients are recorded.\n",
      "Company cherre has error: 'NoneType' object is not iterable\n",
      "Company Vertafore has already been recorded.\n",
      "Company IVANS has already been recorded.\n",
      "Company PIIAC has already been recorded.\n",
      "Company IIANC has already been recorded.\n",
      "Company avyst's clients are recorded.\n",
      "Company D1 Capital Partners has already been recorded.\n",
      "Company Tiger Global has already been recorded.\n",
      "Company S3 Partners has already been recorded.\n",
      "Company CRSP has already been recorded.\n",
      "Company LSEG Data Analytics has already been recorded.\n",
      "Company S&P Global has already been recorded.\n",
      "Company Refinitiv has already been recorded.\n",
      "Company arcana's clients are recorded.\n",
      "Company McKinley Capital Research has already been recorded.\n",
      "Company Wolfe Research has already been recorded.\n",
      "Company Deutsche Bank Markets Research has already been recorded.\n",
      "Company estimize's clients are recorded.\n",
      "Company missionmark's clients are recorded.\n",
      "Company Optimal Blue has already been recorded.\n",
      "Company LoanPASS has already been recorded.\n",
      "Company DocMagic has already been recorded.\n",
      "Company Upstart has already been recorded.\n",
      "Company Willow Servicing has already been recorded.\n",
      "Company Lender Price has already been recorded.\n",
      "Company Beeline has already been recorded.\n",
      "Company National Mortgage Insurance has already been recorded.\n",
      "Company Enact has already been recorded.\n",
      "Company ComplianceEase has already been recorded.\n",
      "Company Advantage Credit has already been recorded.\n",
      "Company MGIC has already been recorded.\n",
      "Company Arch MI has already been recorded.\n",
      "Company CoreLogic has already been recorded.\n",
      "Company Truework has already been recorded.\n",
      "Company First American Data & Analytics has already been recorded.\n",
      "Company Fannie Mae has already been recorded.\n",
      "Company Freddie Mac has already been recorded.\n",
      "Company vesta's clients are recorded.\n",
      "Company Vituity Healthcare has already been recorded.\n",
      "Company Open English has already been recorded.\n",
      "Company Prodege has already been recorded.\n",
      "Company Alector has already been recorded.\n",
      "Company Guardant Health has already been recorded.\n",
      "Company Riskcop Advisory LLC has already been recorded.\n",
      "Company SPS Commerce has already been recorded.\n",
      "Company Change Lending LLC has already been recorded.\n",
      "Company youattest's clients are recorded.\n",
      "Company aer_compliance has error: 'NoneType' object is not iterable\n",
      "Company Integrated Biodiversity Assessment Tool (IBAT) has already been recorded.\n",
      "Company United Nations Environment Programme – World Conservation Monitoring Centre has already been recorded.\n",
      "Company International Union for Conservation of Nature (IUCN) has already been recorded.\n",
      "Company BirdLife International has already been recorded.\n",
      "Company Conservation International has already been recorded.\n",
      "Company naturealpha's clients are recorded.\n",
      "Company Magnit has already been recorded.\n",
      "Company Beeline has already been recorded.\n",
      "Company Intel has already been recorded.\n",
      "Company The Walt Disney Company CIS has already been recorded.\n",
      "Company Samsung has already been recorded.\n",
      "Company Microsoft has already been recorded.\n",
      "Company VimpelCom PJSC has already been recorded.\n",
      "Company Mail.ru Group has already been recorded.\n",
      "Company OMD OM Group has already been recorded.\n",
      "Company aggregion's clients are recorded.\n",
      "Company dexit has error: 'NoneType' object is not iterable\n",
      "Company Houston Lawrence has already been recorded.\n",
      "Company Carter Towler has already been recorded.\n",
      "Company harness_data_intelligence's clients are recorded.\n",
      "Company Montana Bear Properties has already been recorded.\n",
      "Company Fireside RV Rental has already been recorded.\n",
      "Company Stay With Style Scottsdale has already been recorded.\n",
      "Company Williamson RV Rentals has already been recorded.\n",
      "Company Cabo Rentals by Jane has already been recorded.\n",
      "Company Luxury RV Rentals LV has already been recorded.\n",
      "Company GuestSpaces has already been recorded.\n",
      "Company RV Pirates Rentals has already been recorded.\n",
      "Company Stay in the South has already been recorded.\n",
      "Company direct's clients are recorded.\n",
      "Company nexben has error: 'NoneType' object is not iterable\n",
      "Company Point 72 has already been recorded.\n",
      "Company Mozilla has already been recorded.\n",
      "Company Hustle funds has already been recorded.\n",
      "Company Microsoft for Startups has already been recorded.\n",
      "Company Nvidia has already been recorded.\n",
      "Company Xoogler has already been recorded.\n",
      "Company massive's clients are recorded.\n",
      "Company Quickswap has already been recorded.\n",
      "Company DeltaPrime has already been recorded.\n",
      "Company Sceptre has already been recorded.\n",
      "Company SteadeFi has already been recorded.\n",
      "Company Kinetic has already been recorded.\n",
      "Company Hover has already been recorded.\n",
      "Company Cyberscope has already been recorded.\n",
      "Company SaucerSwap has already been recorded.\n",
      "Company Hedera has already been recorded.\n",
      "Company Tolam Earth has already been recorded.\n",
      "Company Tashi has already been recorded.\n",
      "Company Sceptre has already been recorded.\n",
      "Company ledger_works's clients are recorded.\n",
      "Company bankerslab has error: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client_info = read_json_file('data/client_info.json')\n",
    "\n",
    "doc_list = os.listdir('extraction_output_v2')\n",
    "\n",
    "for doc in tqdm(doc_list):\n",
    "    try:\n",
    "        processed_name = doc.replace('_extraction.json', '')\n",
    "        data = read_json_file(f'extraction_output_v2/{doc}')\n",
    "\n",
    "        for client in data['validated_client_descriptions']:\n",
    "            if client['entity_type'] != 'company':\n",
    "                continue\n",
    "            # If a company's name already exists in the dictionary\n",
    "            if client['name'] in client_info:\n",
    "                \n",
    "                # If its service provider does not appear in the saved list, then append it\n",
    "                if processed_name not in client_info[client['name']]['service_provider_processed']:\n",
    "                    client_info[client['name']]['service_provider_processed'].append(processed_name)\n",
    "                    client_info[client['name']]['service_provider'].append(get_additional_info(processed_name, 'companies'))\n",
    "                    client_info[client['name']]['service_provider_url'].append('https://' + get_additional_info(processed_name, 'processed_url'))\n",
    "                else:\n",
    "                    print(f'Company {client[\"name\"]} has already been recorded.')\n",
    "            \n",
    "            # If a company's name already does not exist, add the new company\n",
    "            else:\n",
    "                client_info[client['name']] = {'processed_name': process_company_name(client['name']),\n",
    "                                    'url': client['url'],\n",
    "                                    'service_provider_processed': [processed_name],\n",
    "                                    'service_provider': [get_additional_info(processed_name, 'companies')],\n",
    "                                    'service_provider_url': ['https://' + get_additional_info(processed_name, 'processed_url')]\n",
    "                                    }\n",
    "        print(f\"Company {data['processed_company']}'s clients are recorded.\")\n",
    "    except Exception as e:\n",
    "        print(f'Company {processed_name} has error: {e}')\n",
    "        \n",
    "write_json_file('data/client_info.json', client_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data = read_json_file('data/client_info.json')\n",
    "for company, company_info in tqdm(client_data.items(), desc=\"Scraping data\", position=0, leave=True):\n",
    "    base_url = company_info['url']\n",
    "    if not base_url:\n",
    "        continue\n",
    "    try:\n",
    "        all_urls, related_urls = get_related_urls(base_url)\n",
    "        if len(related_urls) > 10:\n",
    "            related_urls = select_urls(related_urls, 10)\n",
    "        result = crawl_data(base_url, related_urls, f'client_scraping_output/{company_info[\"processed_name\"]}.json', overwrite=False)\n",
    "    except Exception as e:\n",
    "        print(f'Company {company} has error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "client_data = read_json_file('data/client_info.json')\n",
    "doc_list = os.listdir('scraping_output_v2_raw')\n",
    "\n",
    "for company, company_info in client_data.items():\n",
    "    filename = f\"{company_info['processed_name']}.json\"\n",
    "    source_path = f'scraping_output_v2_raw/{filename}'\n",
    "    destination_path = f'client_scraping_output/{filename}'\n",
    "    if filename in doc_list:\n",
    "        shutil.move(source_path, destination_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = os.listdir('scraping_output_v2_raw')\n",
    "'bloomberg.json' in doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
