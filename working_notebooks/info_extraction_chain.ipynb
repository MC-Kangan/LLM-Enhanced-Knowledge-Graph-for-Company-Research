{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-26 13:32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl_scraping import *\n",
    "from utility import *\n",
    "from llm_extraction import *\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import tiktoken as tiktoken\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "print(current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding\n",
    "- gpt-4o: \"o200k_base\",\n",
    "- gpt-4: \"cl100k_base\",\n",
    "- gpt-3.5-turbo: \"cl100k_base\",\n",
    "- gpt-3.5: \"cl100k_base\",  # Common shorthand\n",
    "- gpt-35-turbo : \"cl100k_base\",  # Azure deployment name\n",
    "\n",
    "gpt-4o US$5.00 / 1M input tokens； US$15.00 / 1M output tokens\n",
    "\n",
    "gpt-4o context length: 128K tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>companies</th>\n",
       "      <th>company_former_name</th>\n",
       "      <th>company_legal_name</th>\n",
       "      <th>competitors</th>\n",
       "      <th>description</th>\n",
       "      <th>primary_industry_sector</th>\n",
       "      <th>primary_industry_group</th>\n",
       "      <th>primary_industry_code</th>\n",
       "      <th>all_industries</th>\n",
       "      <th>...</th>\n",
       "      <th>first_financing_valuation</th>\n",
       "      <th>first_financing_valuation_status</th>\n",
       "      <th>last_financing_valuation</th>\n",
       "      <th>last_financing_valuation_status</th>\n",
       "      <th>last_known_valuation</th>\n",
       "      <th>last_known_valuation_date</th>\n",
       "      <th>last_known_valuation_deal_type</th>\n",
       "      <th>processed_url</th>\n",
       "      <th>is_accessible</th>\n",
       "      <th>processed_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55185-04</td>\n",
       "      <td>Estimize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estimize, Inc.</td>\n",
       "      <td>Neudata, SigFig, Motif (Financial Software), Y...</td>\n",
       "      <td>Developer of an open financial estimates platf...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Financial Software</td>\n",
       "      <td>Financial Software*, Media and Information Ser...</td>\n",
       "      <td>...</td>\n",
       "      <td>6.34</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.00</td>\n",
       "      <td>16/07/2015</td>\n",
       "      <td>Early Stage VC</td>\n",
       "      <td>www.estimize.com</td>\n",
       "      <td>True</td>\n",
       "      <td>estimize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56288-62</td>\n",
       "      <td>New Constructs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Constructs, LLC</td>\n",
       "      <td>Morningstar, CFRA, Finbox (Media and Informati...</td>\n",
       "      <td>Operator of an investment research firm intend...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Financial Software</td>\n",
       "      <td>Financial Software*, Media and Information Ser...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.17</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.17</td>\n",
       "      <td>13/05/2003</td>\n",
       "      <td>Early Stage VC</td>\n",
       "      <td>www.newconstructs.com</td>\n",
       "      <td>True</td>\n",
       "      <td>new_constructs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53739-01</td>\n",
       "      <td>Procore Technologies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Procore Technologies, Inc.</td>\n",
       "      <td>Projectmates, eBuilder, CMiC</td>\n",
       "      <td>Procore Technologies Inc is a cloud-based cons...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Business/Productivity Software</td>\n",
       "      <td>Business/Productivity Software*, Construction ...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Actual</td>\n",
       "      <td>8585.03</td>\n",
       "      <td>Estimated</td>\n",
       "      <td>8585.03</td>\n",
       "      <td>20/05/2021</td>\n",
       "      <td>IPO</td>\n",
       "      <td>www.procore.com</td>\n",
       "      <td>True</td>\n",
       "      <td>procore_technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153145-27</td>\n",
       "      <td>Proof</td>\n",
       "      <td>16 Pins, Notarize</td>\n",
       "      <td>Notarize, Inc.</td>\n",
       "      <td>Templafy, ZorroSign, eOriginal, PandaDoc, Cong...</td>\n",
       "      <td>Developer of an identity-assured transaction m...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Business/Productivity Software</td>\n",
       "      <td>Business/Productivity Software*, Media and Inf...</td>\n",
       "      <td>...</td>\n",
       "      <td>46.50</td>\n",
       "      <td>Actual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>760.00</td>\n",
       "      <td>25/03/2021</td>\n",
       "      <td>Later Stage VC</td>\n",
       "      <td>www.proof.com</td>\n",
       "      <td>True</td>\n",
       "      <td>proof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52304-77</td>\n",
       "      <td>SMS Assist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SMS Assist, L.L.C.</td>\n",
       "      <td>ServiceChannel, Divisions Maintenance Group, T...</td>\n",
       "      <td>Provider of business services intended to deli...</td>\n",
       "      <td>Business Products and Services (B2B)</td>\n",
       "      <td>Commercial Services</td>\n",
       "      <td>Other Commercial Services</td>\n",
       "      <td>Buildings and Property, Business/Productivity ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>950.00</td>\n",
       "      <td>Estimated</td>\n",
       "      <td>950.00</td>\n",
       "      <td>05/01/2023</td>\n",
       "      <td>Merger/Acquisition</td>\n",
       "      <td>www.smsassist.com</td>\n",
       "      <td>True</td>\n",
       "      <td>sms_assist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  company_id             companies company_former_name  \\\n",
       "0   55185-04              Estimize                 NaN   \n",
       "1   56288-62        New Constructs                 NaN   \n",
       "3   53739-01  Procore Technologies                 NaN   \n",
       "5  153145-27                 Proof   16 Pins, Notarize   \n",
       "6   52304-77            SMS Assist                 NaN   \n",
       "\n",
       "           company_legal_name  \\\n",
       "0              Estimize, Inc.   \n",
       "1         New Constructs, LLC   \n",
       "3  Procore Technologies, Inc.   \n",
       "5              Notarize, Inc.   \n",
       "6          SMS Assist, L.L.C.   \n",
       "\n",
       "                                         competitors  \\\n",
       "0  Neudata, SigFig, Motif (Financial Software), Y...   \n",
       "1  Morningstar, CFRA, Finbox (Media and Informati...   \n",
       "3                       Projectmates, eBuilder, CMiC   \n",
       "5  Templafy, ZorroSign, eOriginal, PandaDoc, Cong...   \n",
       "6  ServiceChannel, Divisions Maintenance Group, T...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Developer of an open financial estimates platf...   \n",
       "1  Operator of an investment research firm intend...   \n",
       "3  Procore Technologies Inc is a cloud-based cons...   \n",
       "5  Developer of an identity-assured transaction m...   \n",
       "6  Provider of business services intended to deli...   \n",
       "\n",
       "                primary_industry_sector primary_industry_group  \\\n",
       "0                Information Technology               Software   \n",
       "1                Information Technology               Software   \n",
       "3                Information Technology               Software   \n",
       "5                Information Technology               Software   \n",
       "6  Business Products and Services (B2B)    Commercial Services   \n",
       "\n",
       "            primary_industry_code  \\\n",
       "0              Financial Software   \n",
       "1              Financial Software   \n",
       "3  Business/Productivity Software   \n",
       "5  Business/Productivity Software   \n",
       "6       Other Commercial Services   \n",
       "\n",
       "                                      all_industries  ...  \\\n",
       "0  Financial Software*, Media and Information Ser...  ...   \n",
       "1  Financial Software*, Media and Information Ser...  ...   \n",
       "3  Business/Productivity Software*, Construction ...  ...   \n",
       "5  Business/Productivity Software*, Media and Inf...  ...   \n",
       "6  Buildings and Property, Business/Productivity ...  ...   \n",
       "\n",
       "  first_financing_valuation first_financing_valuation_status  \\\n",
       "0                      6.34                           Actual   \n",
       "1                      2.17                           Actual   \n",
       "3                      4.00                           Actual   \n",
       "5                     46.50                           Actual   \n",
       "6                       NaN                              NaN   \n",
       "\n",
       "  last_financing_valuation  last_financing_valuation_status  \\\n",
       "0                      NaN                              NaN   \n",
       "1                      NaN                              NaN   \n",
       "3                  8585.03                        Estimated   \n",
       "5                      NaN                              NaN   \n",
       "6                   950.00                        Estimated   \n",
       "\n",
       "  last_known_valuation last_known_valuation_date  \\\n",
       "0                36.00                16/07/2015   \n",
       "1                 2.17                13/05/2003   \n",
       "3              8585.03                20/05/2021   \n",
       "5               760.00                25/03/2021   \n",
       "6               950.00                05/01/2023   \n",
       "\n",
       "  last_known_valuation_deal_type          processed_url  is_accessible  \\\n",
       "0                 Early Stage VC       www.estimize.com           True   \n",
       "1                 Early Stage VC  www.newconstructs.com           True   \n",
       "3                            IPO        www.procore.com           True   \n",
       "5                 Later Stage VC          www.proof.com           True   \n",
       "6             Merger/Acquisition      www.smsassist.com           True   \n",
       "\n",
       "         processed_name  \n",
       "0              estimize  \n",
       "1        new_constructs  \n",
       "3  procore_technologies  \n",
       "5                 proof  \n",
       "6            sms_assist  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('data/PitchBook_All_Columns_2024_07_04_14_48_36_accessibility.csv')\n",
    "df_all = df_all[~df_all['business_status'].isin(['Out of Business', 'Bankruptcy: Liquidation', 'Bankruptcy: Admin/Reorg'])]\n",
    "df_all['companies'] = df_all['companies'].str.replace(r'\\s*\\(.*?\\)\\s*', '', regex=True)\n",
    "df_all = df_all[df_all['is_accessible'] == True]\n",
    "df_all['processed_name'] = df_all['companies'].apply(process_company_name)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of first shorten the page by extracting relevant information\n",
    "Issue: The output of the content might be shorten too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_summary(text, model_name=\"gpt-4o\"):\n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to extract information \n",
    "    from the given text and convert it into a text (string) format. \n",
    "    The output response should contain only the data extracted from the text, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing. Do not hallucinate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the extraction prompt\n",
    "    extraction_prompt = \"\"\"\n",
    "    You are provided with a text obtained from a company's webpage. Your task is to extract any sections or paragraphs that are relevant to the specified information of interest.\n",
    "\n",
    "    ## Information of Interest:\n",
    "\n",
    "    1. **About Product or Service**:\n",
    "    - Any details about the products or services the company offers, including their features.\n",
    "\n",
    "    2. **About Partner or Client**:\n",
    "    - Any information about the company's partners or clients.\n",
    "    - Any use cases (case studies) describing how a client is using the company's product or service.\n",
    "    \n",
    "    ## Note:\n",
    "    Sometimes, the company does not explicit describe their clients and the client use case, instead, they will only display clients' logos. \n",
    "    You then need to extract client's name from their logos. \n",
    "    \n",
    "    ## Instructions:\n",
    "    - Do not summarize the content. Extract the raw lines or sections as they are.\n",
    "    - If you are unsure about the relevance of the information, include it to ensure comprehensive coverage.\n",
    "    - Output the extracted information in standard text format.\n",
    "\n",
    "    ## Examples:\n",
    "\n",
    "    ### Example 1: Product or Service\n",
    "    If the input text contains:\n",
    "    \"Our company offers innovative cloud solutions that help businesses streamline their operations. Our key features include scalability, security, and ease of use.\n",
    "    We partner with leading firms such as TechCorp and SoftInc to deliver top-notch services.\"\n",
    "\n",
    "    The output should be:\n",
    "    \"Our company offers innovative cloud solutions that help businesses streamline their operations. Our key features include scalability, security, and ease of use.\n",
    "    We partner with leading firms such as TechCorp and SoftInc to deliver top-notch services.\"\n",
    "\n",
    "    ### Example 2: Client Logos\n",
    "    If the input text contains:\n",
    "    \"Our platform and service is trusted by these innovative companies:\n",
    "    ![Nationwide Logo]\n",
    "    ![Freedom 365 Logo]\n",
    "    ![Bestow Logo]\n",
    "    ...\"\n",
    "    \n",
    "    The output should be:\n",
    "    \"Our platform and service is trusted by these innovative companies: \n",
    "    Clients are: Nationwide, Freedom 365, Bestow...\"\n",
    "   \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_message),\n",
    "            (\"system\", extraction_prompt),\n",
    "            (\"human\", \"Use the given text to extract information: {input}\"),\n",
    "            (\"human\", \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                ## Rules:\n",
    "                - Make sure to answer in the standard text format.\n",
    "                - If no information is provided, return nothing.\n",
    "                - DO NOT HALLUCINATE.\n",
    "             \"\"\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(openai_api_key=os.getenv('OPENAI_KEY'),\n",
    "                    temperature=0, \n",
    "                    model_name=model_name)\n",
    "\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = llm_chain.invoke({'input': text})\n",
    "    \n",
    "    return response\n",
    "\n",
    "def llm_summary_execution(processed_name:str, \n",
    "                          scrape_file_path:str,\n",
    "                          summary_file_path:str,\n",
    "                          overwrite:bool = False, \n",
    "                          model_name:str = 'gpt-4o-mini'):\n",
    "\n",
    "    scrape_data = read_json_file(scrape_file_path)\n",
    "    \n",
    "    file_modified = False\n",
    "\n",
    "    # Load existing data if the file exists\n",
    "    if os.path.exists(summary_file_path):\n",
    "        with open(summary_file_path, 'r') as file:\n",
    "            extracted_data = json.load(file)\n",
    "    else:\n",
    "        extracted_data = {}\n",
    "\n",
    "    for endpoint, content in tqdm(scrape_data.items(), total=len(scrape_data), desc=\"Extracting data\", position=0, leave=True):\n",
    "        if endpoint in ['timestamp', 'processed_company', 'url']:\n",
    "            continue\n",
    "        if endpoint in extracted_data and not overwrite:\n",
    "            print(f\"Company: {processed_name}; Skipping {endpoint} as it already exists and overwrite is set to False.\")\n",
    "            continue  # Skip this URL and move to the next one\n",
    "        else:\n",
    "            clean_content = clean_scraped_content(content)\n",
    "            extracted_data[endpoint] = llm_summary(text = clean_content, model_name = model_name)\n",
    "            print(f'Company: {processed_name}; Content in {endpoint} is extracted.')\n",
    "            \n",
    "            current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "            extracted_data['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\") + ' Etc/GMT'\n",
    "            file_modified = True\n",
    "    \n",
    "    if file_modified:\n",
    "        extracted_data['processed_company'] = processed_name\n",
    "        extracted_data['url'] = scrape_data['url']\n",
    "        write_json_file(summary_file_path, extracted_data)\n",
    "        \n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  31%|███       | 4/13 [00:09<00:22,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#partner-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  38%|███▊      | 5/13 [00:15<00:26,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#rev-plus is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  46%|████▌     | 6/13 [00:20<00:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /customer-agreement is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  54%|█████▍    | 7/13 [00:26<00:26,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#hotel-it is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  62%|██████▏   | 8/13 [00:32<00:24,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  69%|██████▉   | 9/13 [00:39<00:21,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#accounting-services is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  77%|███████▋  | 10/13 [00:45<00:16,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#basic-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  85%|████████▍ | 11/13 [00:50<00:11,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /partner-plan is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  92%|█████████▏| 12/13 [00:58<00:06,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in main_page is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|██████████| 13/13 [01:04<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: the_booking_factory; Content in /services#bf-web is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_name = 'the_booking_factory'\n",
    "scrape_file_path = f'scraping_output_v2_raw/{processed_name}.json'\n",
    "summary_file_path = f'extraction_summary_v2/{processed_name}_summary_str.json'\n",
    "\n",
    "response = llm_summary_execution(processed_name = processed_name,\n",
    "                                 scrape_file_path = scrape_file_path,\n",
    "                                 summary_file_path = summary_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor\n",
    "\n",
    "https://github.com/jxnl/instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "class ProductDescription(BaseModel):\n",
    "    name: str = Field(..., alias='summarised name of product')\n",
    "    description: str = Field(..., alias='concise features description of the product or service')\n",
    "\n",
    "class SummaryProductDescription(BaseModel):\n",
    "    name: str = Field(..., alias='summarised name of the main product offerings of the company')\n",
    "    description: str = Field(..., alias='summary of product offering of the company')\n",
    "\n",
    "class ClientDescription(BaseModel):\n",
    "    name: str = Field(..., alias='name of the client or partner')\n",
    "    description: Optional[str] = Field(None, alias='description of the usecase')\n",
    "\n",
    "class ExtractedInformation(BaseModel):\n",
    "    product_descriptions: Optional[List[ProductDescription]] = None\n",
    "    summary_product_description: Optional[SummaryProductDescription] = None\n",
    "    client_descriptions: Optional[List[ClientDescription]] = None\n",
    "    \n",
    "class ValidatedClientDescription(BaseModel):\n",
    "    name: str = Field(..., alias='name of the client or partner')\n",
    "    entity_type: Literal[\"person\", \"company\", \"general_entity\", \"other\", \"school\"]\n",
    "    product_used: Optional[str] = Field(None, alias='summary of the product or service used by the client or partner')\n",
    "    description: Optional[str] = Field(None, alias='description of the usecase')\n",
    "\n",
    "class ValidatedExtractedInformation(BaseModel):\n",
    "    # product_descriptions: Optional[List[ProductDescription]] = None\n",
    "    # summary_product_description: Optional[SummaryProductDescription] = None\n",
    "    client_descriptions: Optional[List[ValidatedClientDescription]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def initial_extraction(text: str, model_name: str = 'gpt-4o', additional_context: str = None) -> ExtractedInformation:\n",
    "    \n",
    "    # Patch the OpenAI client with Instructor\n",
    "    client = instructor.from_openai(OpenAI(api_key=os.getenv('OPENAI_KEY')))\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to extract structured information \n",
    "    from the given text and convert it into a structured format. \n",
    "    The output response should contain only the data extracted from the text, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing for that field. Do not hallucinate.\n",
    "    \"\"\"\n",
    "    \n",
    "    custom_extraction_prompt = \"\"\"\n",
    "    Extract the following information from the text extracted from a webpage of a company:\n",
    "\n",
    "    1. Product Description:\n",
    "    - What service or product does the company provide?\n",
    "    - What features does the product or service have?\n",
    "    Note: If the company has more than one product or service, automatically detect and list each product with its relevant details.\n",
    "    \n",
    "    2. Summary of Product Offering:\n",
    "    - Summary of the description of the service that the company provide, taking into consideration of all the product offerings.\n",
    "    Note: Do not include any company-specific information in the summary, such as company name and location.\n",
    "    \n",
    "    3. Client Description:\n",
    "    - Name of the corporate client or partner. \n",
    "    - Description of the use case.\n",
    "    Note: Focus on the extraction of company's name, instead of individuals.\n",
    "    Note: If the description of the use case is not mentioned, it should be None.\n",
    "    \n",
    "\n",
    "    Output in a structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    rule_prompt = \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                    ## Rules:\n",
    "                    - The aim is to achieve simplicity and clarity in the extracted text.\n",
    "                    - Make sure to answer in the structured format.\n",
    "                    - If no information is provided for any of the fields, return nothing of that field.\n",
    "                    - DO NOT HALLUCINATE.\n",
    "                \"\"\"\n",
    "    \n",
    "    extraction_prompt = f\"\"\"\n",
    "    {system_message}\n",
    "    {custom_extraction_prompt}\n",
    "    \"\"\"\n",
    "    \n",
    "    if additional_context:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            response_model=ExtractedInformation,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": extraction_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Use the given text to extract information: {text}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Here are some additional descriptions about this company for your reference:\n",
    "                                                {additional_context}\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": rule_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            response_model=ExtractedInformation,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": extraction_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Use the given text to extract information: {text}\"},\n",
    "                {\"role\": \"user\", \"content\": rule_prompt}\n",
    "            ]\n",
    "        )\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_validation(products: list, clients: list, summary: dict, model_name: str = 'gpt-4o') -> ValidatedExtractedInformation:\n",
    "    \n",
    "    # Patch the OpenAI client with Instructor\n",
    "    client = instructor.from_openai(OpenAI(api_key=os.getenv('OPENAI_KEY')))\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "    You are an intelligent text extraction and conversion assistant. Your task is to validate the client information, classify the client names into different entity types, and determine which product is likely used by the client. \n",
    "    The output response should contain only the data validated and assigned, with no additional commentary, explanations, or extraneous information.\n",
    "    If the required information could not be found from the given source, return nothing for that field. Do not hallucinate.\n",
    "    \"\"\"\n",
    "    \n",
    "    product_info = \"\\n\".join([f\"Product: {p['name']}; Description: {p['description']}\" for p in products])\n",
    "    client_info = \"\\n\".join([f\"Client: {c['name']}; Description: {c['description']}\" for c in clients])\n",
    "    summary_info = f\"{summary['name']}: {summary['description']}\"\n",
    "    \n",
    "    few_shot_examples = \"\"\"\n",
    "        ## Example 1:\n",
    "        Client Name: Mike Johnson, CEO of TechCorp\n",
    "        Entity_type: person\n",
    "        - Reason: Mike Johnson is the name of a person. \n",
    "        \n",
    "        ## Example 2:\n",
    "        Client Name: Government\n",
    "        Entity_type: general_entity\n",
    "        - Reason: \"Government\" is a general entity, not a specific company.\n",
    "\n",
    "        ## Example 3:\n",
    "        Client Name: Innovative Solutions LLC\n",
    "        Entity_type: company\n",
    "        - Reason: Innovative Solutions LLC is a specific company name.\n",
    "        \n",
    "        ## Example 4:\n",
    "        Client Name: A US resort\n",
    "        Entity_type: general_entity\n",
    "        - Reason: \"A US resort\" is a general description, not a specific company name.\n",
    "    \n",
    "        ## Example 5: \n",
    "        Client Name: University College London\n",
    "        Entity_type: school\n",
    "        - Reason: University College London is a specific school name.\n",
    "    \"\"\"\n",
    "\n",
    "    validation_prompt = f\"\"\"\n",
    "    {system_message}\n",
    "    Here is the product information extracted:\n",
    "    {product_info}\n",
    "    \n",
    "    Here is the summary of product offerings of the company:\n",
    "    {summary_info}\n",
    "    \n",
    "    Here are the clients and their use cases:\n",
    "    {client_info}\n",
    "    \n",
    "    Your task is to:\n",
    "    1. Classify each client name into one of the following entity types: person, company, general_entity, school, or other.\n",
    "       Note: the entity type \"company\" should be given to specific companies, with company names.\n",
    "    2. Based on the product descriptions and client use cases, assign the most likely product used by each client. \n",
    "       If you are not confident about which product the client uses, return None for that field.\n",
    "\n",
    "    Here are some examples regarding the classifying clients into different entity types:\n",
    "    {few_shot_examples}\n",
    "\n",
    "    Output in a structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        response_model=ValidatedExtractedInformation,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": validation_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "                Here are the rules that you need to adhere:\n",
    "                ## Rules:\n",
    "                - Classify each client name into one of the following entity types: person, company, general_entity, school, or other.\n",
    "                - Assign the most likely product used by each client based on the provided product descriptions and use cases.\n",
    "                - If the product used is not clear, return None for that field.\n",
    "                - Make sure to answer in the structured format.\n",
    "                - DO NOT HALLUCINATE.\n",
    "            \"\"\"},\n",
    "        ]\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_extraction_execution(processed_name:str, \n",
    "                             summary_file_path:str,\n",
    "                             extraction_file_path:str, \n",
    "                             include_additional_context:bool = True, \n",
    "                             overwrite:bool = False):\n",
    "    \n",
    "    if not overwrite and os.path.exists(extraction_file_path):\n",
    "        print(f\"Company: {processed_name}; Skipping extraction as the extraction file already exists and overwrite is set to False.\")\n",
    "        return None\n",
    "    else:\n",
    "        if os.path.exists(summary_file_path):\n",
    "            summary = read_json_file(summary_file_path)\n",
    "\n",
    "            combined_summary = f\"## Main Page:\\n {summary['main_page']}\\n----------------\\n\"\n",
    "\n",
    "            for endpoint, text in summary.items():\n",
    "                if endpoint not in [\"main_page\", \"timestamp\", \"processed_company\", \"url\"]:\n",
    "                    combined_summary += f\"## {endpoint}:\\n{text}\\n----------------\\n\"\n",
    "            \n",
    "            print(f\"Company: {processed_name}; Information extraction begins.\")\n",
    "            if include_additional_context:\n",
    "                context = get_additional_info(processed_name, 'description')\n",
    "                \n",
    "                print(f'Company: {processed_name}; Estimated Cost: ${calculate_cost(combined_summary + context)}')\n",
    "                print(f'Company: {processed_name}; Pitchbook description obtained: {context}')\n",
    "                \n",
    "                initial_response = initial_extraction(text = combined_summary, \n",
    "                                                additional_context = context).dict()\n",
    "                \n",
    "            else:\n",
    "                print(f'Company: {processed_name}; Estimated Cost: ${calculate_cost(combined_summary)}')\n",
    "                initial_response = initial_extraction(text = combined_summary, \n",
    "                                            additional_context = None).dict()\n",
    "            \n",
    "            print(f'Company: {processed_name}; PART 1 - Initial extraction is completed.')\n",
    "            \n",
    "            result = initial_response\n",
    "            \n",
    "            if initial_response['client_descriptions']:\n",
    "                products = initial_response['product_descriptions'] if initial_response['product_descriptions'] else []\n",
    "                clients = initial_response['client_descriptions'] if initial_response['client_descriptions'] else []\n",
    "                summary = initial_response['summary_product_description']\n",
    "\n",
    "                validated_response = information_validation(products, clients, summary)\n",
    "                print(f'Company: {processed_name}; PART 2 - Information validation is completed.')\n",
    "                result['validated_client_descriptions'] = validated_response.dict()['client_descriptions']\n",
    "                \n",
    "            else:\n",
    "                print(f'Company: {processed_name}; PART 2 - Skipped, due to lack of client information.')\n",
    "                result['validated_client_descriptions'] = None\n",
    "            \n",
    "            current_dateTime = datetime.now(pytz.timezone('Etc/GMT'))\n",
    "            result['timestamp'] = current_dateTime.strftime(format = \"%Y-%m-%d %H:%M\") + ' Etc/GMT'\n",
    "            result['processed_company'] = processed_name\n",
    "            result['url'] = read_json_file(summary_file_path)['url']\n",
    "\n",
    "            write_json_file(extraction_file_path, result)\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(f'Summary file: {summary_file_path} does not exist.')\n",
    "            return None\n",
    "\n",
    "def add_client_url_to_extraction_output(processed_name:str, extraction_file_path:str, verbose:bool = False, overwrite:bool = False):\n",
    "    data = read_json_file(extraction_file_path)\n",
    "    \n",
    "    # Check if the company has any clients\n",
    "    if data['validated_client_descriptions']:\n",
    "        \n",
    "        # If the url key already exists, meaning the urls have been added and overwrite is False\n",
    "        if 'url' in data['validated_client_descriptions'][0].keys() and not overwrite:\n",
    "            print(f\"Company: {processed_name}; Skipping as the clients' URLs have been added and overwrite is set to False.\")\n",
    "        else:\n",
    "            for client in data['validated_client_descriptions']:\n",
    "                if client['entity_type'] != 'company':\n",
    "                    client['url'] = None\n",
    "                else:\n",
    "                    url = get_and_verify_company_link(client['name'], verbose = verbose)\n",
    "                    client['url'] = url\n",
    "            print(f\"Company: {processed_name}; Client is extracted.\")\n",
    "    else:\n",
    "        print(f\"Company: {processed_name}; No clients' information.\")\n",
    "    write_json_file(extraction_file_path, data)\n",
    "    return None\n",
    "\n",
    "    \n",
    "def get_embedding(text:str, embedding_model:str=\"text-embedding-3-small\"):\n",
    "   client_openai = OpenAI(api_key=os.getenv('OPENAI_KEY'))\n",
    "   \n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client_openai.embeddings.create(input = [text], model=embedding_model).data[0].embedding\n",
    "\n",
    "\n",
    "def get_product_embedding(processed_name:str, extraction_file_path:str, embedding_model:str=\"text-embedding-3-small\"):\n",
    "    \n",
    "    data = read_json_file(extraction_file_path)\n",
    "    # Check wheather embedding has already been done\n",
    "    if 'name_embedding' in data['summary_product_description']:\n",
    "        print(f'Company: {processed_name}; Embedding has already been done.')\n",
    "        pass\n",
    "    else:\n",
    "        product_lst = data['product_descriptions']\n",
    "        for product in product_lst:\n",
    "            product['description_embedding'] = get_embedding(text = product['description'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "            product['name_embedding'] = get_embedding(text = product['name'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "\n",
    "        summary_product = data['summary_product_description']\n",
    "        summary_product['description_embedding'] = get_embedding(text = summary_product['description'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "        summary_product['name_embedding'] = get_embedding(text = summary_product['name'],\n",
    "                                                                embedding_model = embedding_model)\n",
    "        print(f'Company: {processed_name}; Embedding is completed.')\n",
    "        write_json_file(extraction_file_path, data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def update_client_list(processed_name:str, extraction_file_path:str, client_file_path:str = 'data/client_info.json', verbose:bool = False):\n",
    "    \n",
    "    data = read_json_file(extraction_file_path)\n",
    "    client_info = read_json_file(client_file_path)\n",
    "        \n",
    "    if data['validated_client_descriptions']:\n",
    "        try:        \n",
    "            for client in data['validated_client_descriptions']:\n",
    "                if client['entity_type'] != 'company':\n",
    "                    continue\n",
    "                # If a company's name already exists in the dictionary and the url is unchanged\n",
    "                if client['name'] in client_info and client['url'] == client_info[client['name']]['url'] :\n",
    "                    # If its service provider does not appear in the saved list, then append it\n",
    "                    if processed_name not in client_info[client['name']]['service_provider_processed']:\n",
    "                        client_info[client['name']]['service_provider_processed'].append(processed_name)\n",
    "                        client_info[client['name']]['service_provider'].append(get_additional_info(processed_name, 'companies'))\n",
    "                        client_info[client['name']]['service_provider_url'].append('https://' + get_additional_info(processed_name, 'processed_url'))\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f'Company {client[\"name\"]} has already been recorded.')\n",
    "                \n",
    "                # If a company's name already does not exist, add the new company\n",
    "                else:\n",
    "                    client_info[client['name']] = {'processed_name': process_company_name(client['name']),\n",
    "                                        'url': client['url'],\n",
    "                                        'service_provider_processed': [processed_name],\n",
    "                                        'service_provider': [get_additional_info(processed_name, 'companies')],\n",
    "                                        'service_provider_url': ['https://' + get_additional_info(processed_name, 'processed_url')]\n",
    "                                        }\n",
    "            print(f\"Company: {data['processed_company']}; Clients information is updated.\")\n",
    "            write_json_file(client_file_path, client_info)\n",
    "        except Exception as e:\n",
    "            print(f'Company: {processed_name}; Error occurred: {e}')\n",
    "    else:\n",
    "        print(f'Company: {processed_name}; No clients to be updated')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  57%|█████▋    | 4/7 [00:05<00:04,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: auquan; Content in main_page is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  71%|███████▏  | 5/7 [00:10<00:04,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: auquan; Content in /professional-services is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:  86%|████████▌ | 6/7 [00:19<00:04,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: auquan; Content in /success-cases/ is extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|██████████| 7/7 [00:26<00:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: auquan; Content in /success-cases is extracted.\n",
      "Company: auquan; Information extraction begins.\n",
      "Company: auquan; Estimated Cost: $0.010124999999999999\n",
      "Company: auquan; Pitchbook description obtained: Developer of a data science platform intended to discover and implement newer trading ideas. The company's platform develops high-quality trading strategies and bridges the gap between data science and finance, enabling clients to translate the analytical skills of talented people into trading profits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: auquan; PART 1 - Initial extraction is completed.\n",
      "Company: auquan; PART 2 - Information validation is completed.\n",
      "Company: auquan; Client is extracted.\n",
      "Company: auquan; Embedding is completed.\n",
      "Company: auquan; Clients information is updated.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_name = 'auquan'\n",
    "scrape_file_path = f'scraping_output_v2_raw/{processed_name}.json'\n",
    "summary_file_path = f'extraction_summary_v2/{processed_name}_summary_str.json'\n",
    "extraction_file_path = f'extraction_output_v2/{processed_name}_extraction.json'\n",
    "\n",
    "_ = llm_summary_execution(processed_name = processed_name,\n",
    "                                 scrape_file_path = scrape_file_path,\n",
    "                                 summary_file_path = summary_file_path)\n",
    "\n",
    "_ = llm_extraction_execution(processed_name = processed_name,\n",
    "                         summary_file_path = summary_file_path,\n",
    "                         extraction_file_path = extraction_file_path, \n",
    "                         include_additional_context = True, \n",
    "                         overwrite = False)\n",
    "\n",
    "_ = add_client_url_to_extraction_output(processed_name = processed_name,\n",
    "                                    extraction_file_path = extraction_file_path)\n",
    "\n",
    "_ = get_product_embedding(processed_name = processed_name,\n",
    "                      extraction_file_path = extraction_file_path)\n",
    "\n",
    "_ = update_client_list(processed_name = processed_name,\n",
    "                   extraction_file_path = extraction_file_path,\n",
    "                   client_file_path = 'data/client_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = os.listdir('extraction_summary_v2')\n",
    "for doc in doc_list:\n",
    "    processed_name = doc.replace('_summary_str.json', '')\n",
    "    summary_file_path = f'extraction_summary_v2/{processed_name}_summary_str.json'\n",
    "    extraction_file_path = f'extraction_output_v2/{processed_name}_extraction.json'\n",
    "    try:\n",
    "        _ = llm_extraction_execution(processed_name = processed_name,\n",
    "                                summary_file_path = summary_file_path,\n",
    "                                extraction_file_path = extraction_file_path, \n",
    "                                include_additional_context = True, \n",
    "                                overwrite = False)\n",
    "\n",
    "        _ = get_product_embedding(processed_name = processed_name,\n",
    "                            extraction_file_path = extraction_file_path)\n",
    "\n",
    "        _ = add_client_url_to_extraction_output(processed_name = processed_name,\n",
    "                                            extraction_file_path = extraction_file_path, overwrite = False)\n",
    "        _ = update_client_list(processed_name = processed_name,\n",
    "                        extraction_file_path = extraction_file_path,\n",
    "                        client_file_path = 'data/client_info.json')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error occured on company {processed_name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client data crawling\n",
    "\n",
    "client_data = read_json_file('data/client_info.json')\n",
    "for base_url in tqdm(list(client_data.keys())[700:900], desc=\"Scraping data\", position=0, leave=True):\n",
    "    if not base_url:\n",
    "        continue\n",
    "    if base_url in ['https://www.autodesk.com']:\n",
    "        continue\n",
    "    try:\n",
    "        all_urls, related_urls = get_related_urls(base_url)\n",
    "        if len(related_urls) > 10:\n",
    "            related_urls = select_urls(related_urls, 10)\n",
    "            \n",
    "        processed_name = client_data[base_url][\"processed_name\"]\n",
    "        \n",
    "        if os.path.exists(f'client_scraping_output/{processed_name}.json'):\n",
    "            data = read_json_file(f'client_scraping_output/{processed_name}.json')\n",
    "            scraped_urls = [i for i in list(data.keys()) if i not in [\"processed_company\", \"url\", \"timestamp\"]]\n",
    "            if len(scraped_urls) >= 10 and 'main_page' in scraped_urls:\n",
    "                print(f'Company {processed_name} has already collected 10 pages')\n",
    "                continue\n",
    "            \n",
    "        result = crawl_data(base_url, related_urls, f'client_scraping_output/{client_data[base_url][\"processed_name\"]}.json', overwrite=False)\n",
    "    except Exception as e:\n",
    "        print(f'Company {client_data[base_url][\"processed_name\"]} has error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Set the timeout limit (in seconds)\n",
    "TIMEOUT_LIMIT = 180  # 3 minutes\n",
    "\n",
    "client_data = read_json_file('data/client_info.json')\n",
    "for base_url in tqdm(list(client_data.keys())[1000:1600], desc=\"Scraping data\", position=0, leave=True):\n",
    "    if not base_url:\n",
    "        continue\n",
    "    if base_url in ['https://www.autodesk.com', 'https://www.docuflow.co.uk']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Set the signal handler and a timeout limit\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(TIMEOUT_LIMIT)\n",
    "\n",
    "        all_urls, related_urls = get_related_urls(base_url)\n",
    "        if len(related_urls) > 10:\n",
    "            related_urls = select_urls(related_urls, 10)\n",
    "\n",
    "        processed_name = client_data[base_url][\"processed_name\"]\n",
    "\n",
    "        if os.path.exists(f'client_scraping_output/{processed_name}.json'):\n",
    "            data = read_json_file(f'client_scraping_output/{processed_name}.json')\n",
    "            scraped_urls = [i for i in list(data.keys()) if i not in [\"processed_company\", \"url\", \"timestamp\"]]\n",
    "            if len(scraped_urls) >= 10 and 'main_page' in scraped_urls:\n",
    "                print(f'Company {processed_name} has already collected 10 pages')\n",
    "                continue\n",
    "\n",
    "        result = crawl_data(base_url, related_urls, f'client_scraping_output/{client_data[base_url][\"processed_name\"]}.json', overwrite=False)\n",
    "\n",
    "        # Disable the alarm after successful completion\n",
    "        signal.alarm(0)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f'Timed out scraping {base_url}, moving to the next URL.')\n",
    "    except Exception as e:\n",
    "        print(f'Company {client_data[base_url][\"processed_name\"]} encountered an error: {e}')\n",
    "\n",
    "    finally:\n",
    "        # Ensure the alarm is disabled in case of other exceptions\n",
    "        signal.alarm(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline for clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data: 100%|██████████| 7/7 [00:00<00:00, 85349.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: holded; Skipping main_page as it already exists and overwrite is set to False.\n",
      "Company: holded; Skipping /professional-services as it already exists and overwrite is set to False.\n",
      "Company: holded; Skipping /success-cases/ as it already exists and overwrite is set to False.\n",
      "Company: holded; Skipping /success-cases as it already exists and overwrite is set to False.\n",
      "Company: holded; Information extraction begins.\n",
      "Company: holded; Estimated Cost: $0.01019\n",
      "Company: holded; PART 1 - Initial extraction is completed.\n",
      "Company: holded; PART 2 - Information validation is completed.\n",
      "Company: holded; Client is extracted.\n",
      "Company: holded; Embedding is completed.\n",
      "Company: holded; Error occurred: can only concatenate str (not \"NoneType\") to str\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "processed_name = 'holded'\n",
    "scrape_file_path = f'client_scraping_output/{processed_name}.json'\n",
    "summary_file_path = f'client_extraction_summary/{processed_name}_summary.json'\n",
    "extraction_file_path = f'client_extraction_output/{processed_name}_extraction.json'\n",
    "\n",
    "_ = llm_summary_execution(processed_name = processed_name,\n",
    "                                 scrape_file_path = scrape_file_path,\n",
    "                                 summary_file_path = summary_file_path,\n",
    "                                 overwrite = False)\n",
    "\n",
    "_ = llm_extraction_execution(processed_name = processed_name,\n",
    "                         summary_file_path = summary_file_path,\n",
    "                         extraction_file_path = extraction_file_path, \n",
    "                         include_additional_context = False, \n",
    "                         overwrite = False)\n",
    "\n",
    "_ = add_client_url_to_extraction_output(processed_name = processed_name,\n",
    "                                    extraction_file_path = extraction_file_path)\n",
    "\n",
    "_ = get_product_embedding(processed_name = processed_name,\n",
    "                      extraction_file_path = extraction_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token reduction from post processing raw scraped contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = os.listdir('scraping_output_v2_raw')\n",
    "\n",
    "count = 0\n",
    "doc_index = 0\n",
    "\n",
    "original_tok_lst = []\n",
    "clean_tok_lst = []\n",
    "proportion_lst = []\n",
    "\n",
    "\n",
    "for index in range(500):\n",
    "    \n",
    "    doc = doc_list[index]\n",
    "    if doc == '.DS_Store':\n",
    "        continue\n",
    "    processed_name = doc.replace('.json', '')\n",
    "    data = read_json_file(f'scraping_output_v2_raw/{processed_name}.json')\n",
    "    original_tok = count_tokens(data['main_page'])\n",
    "    clean_tok = count_tokens(clean_scraped_content(data['main_page']))\n",
    "    \n",
    "    original_tok_lst.append(original_tok)\n",
    "    clean_tok_lst.append(clean_tok)\n",
    "    proportion = clean_tok/(original_tok+0.001)\n",
    "    proportion_lst.append(proportion)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5672967198317319"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-sum(proportion_lst)/len(proportion_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.432703280168268"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_array = np.array(proportion_lst)\n",
    "proportion_array.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
